{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d30b9e-ddf3-492b-a4ef-8e6989236f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabularies loaded.\n",
      "Vocabulary sizes:\n",
      "vocab_note: 94\n",
      "vocab_bar: 66\n",
      "vocab_key: 26\n",
      "vocab_tempo: 86\n",
      "vocab_velocity: 129\n",
      "vocab_chord: 62\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from fractions import Fraction\n",
    "import collections\n",
    "\n",
    "# ==========================\n",
    "# Part 1. REMI to numpy\n",
    "# ==========================\n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = ['<pad>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq and token != '<pad>'\n",
    "        ]\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple, np.ndarray)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.token_to_idx.get(token, self.unk) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self.token_to_idx.get('<unk>', 1)  # Returns index 1 by default if '<unk>' is not in the vocabulary\n",
    "\n",
    "# ==========================\n",
    "# 2. Definition of IntegratedRemi2Np Class (No Changes)\n",
    "# ==========================\n",
    "class IntegratedRemi2Np:\n",
    "    def __init__(self, vocabs, num_max_bars=64, num_input=5, num_output=2):\n",
    "        self.vocabs = vocabs\n",
    "        self.num_max_bars = num_max_bars\n",
    "        self.num_steps = num_max_bars * 16  # Quantize one bar into 16 units\n",
    "        self.num_input = num_input  # note, bar, key, tempo, velocity\n",
    "        self.num_output = num_output  # chord, emotion\n",
    "\n",
    "    def process_all_files(self, filepaths):\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "\n",
    "        for filepath in filepaths:\n",
    "            remi_data = self.load_remi_data(filepath)\n",
    "            x, y = self.preprocess(remi_data)\n",
    "            x_data.append(x)\n",
    "            y_data.append(y)\n",
    "\n",
    "        print(\"Conversion completed\")\n",
    "        x_stacked = np.vstack(x_data)\n",
    "        y_stacked = np.vstack(y_data)\n",
    "\n",
    "        return x_stacked, y_stacked\n",
    "\n",
    "    def load_remi_data(self, filepath):\n",
    "        # Function to read REMI data and return it in dictionary form\n",
    "        remi_data = {\n",
    "            \"melody\": self.parse_remi_file(filepath, event_types=[\"Bar\", \"Position\", \"Note On\"]),\n",
    "            \"velocity\": self.parse_remi_file(filepath, event_types=[\"Bar\", \"Position\", \"Note Velocity\"]),\n",
    "            \"tempo\": self.parse_remi_file(filepath, event_types=[\"Tempo Value\"]),\n",
    "            \"chords\": self.parse_remi_file(filepath, event_types=[\"Bar\", \"Position\", \"Chord\"]),\n",
    "            \"key\": self.parse_remi_file(filepath, event_types=[\"Bar\", \"Position\", \"Key\"]),\n",
    "            \"emotion\": self.parse_remi_file(filepath, event_types=[\"Emotion\"])  # Parsing Emotion events\n",
    "        }\n",
    "        return remi_data\n",
    "\n",
    "    def parse_remi_file(self, filepath, event_types):\n",
    "        events = []\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                for event_type in event_types:\n",
    "                    if f\"name={event_type}\" in line:\n",
    "                        events.append(self.parse_event(line))\n",
    "        return events\n",
    "\n",
    "    def parse_event(self, line):\n",
    "        event = {}\n",
    "\n",
    "        # Extract name\n",
    "        if \"name=\" in line:\n",
    "            event_name = line.split(\"name=\")[1].split(\",\")[0].strip()\n",
    "            event['name'] = event_name\n",
    "\n",
    "        # Extract value\n",
    "        if \"value=\" in line:\n",
    "            event_value = line.split(\"value=\")[1].split(\",\")[0].strip()\n",
    "            event['value'] = event_value\n",
    "\n",
    "        # Extract text\n",
    "        if \"text=\" in line:\n",
    "            event_text = line.split(\"text=\")[1].strip()\n",
    "            event_text = event_text.rstrip(')')\n",
    "            event['text'] = event_text\n",
    "\n",
    "        # Extract time\n",
    "        if \"time=\" in line:\n",
    "            event_time = line.split(\"time=\")[1].split(\",\")[0].strip()\n",
    "            event['time'] = int(event_time)\n",
    "\n",
    "        return event\n",
    "\n",
    "    def preprocess(self, remi_data):\n",
    "        # Calculate the actual length of the song\n",
    "        total_bars = self.get_total_bars(remi_data['melody'])\n",
    "        actual_num_steps = min(total_bars * 16, self.num_steps)\n",
    "\n",
    "        # Initialization: Create numpy arrays filled with <pad>\n",
    "        x_sequence = np.full((self.num_steps, self.num_input), self.vocabs['vocab_note'].token_to_idx['<pad>'], dtype=int)\n",
    "        y_sequence = np.full((self.num_steps, self.num_output), self.vocabs['vocab_chord'].token_to_idx['<pad>'], dtype=int)\n",
    "\n",
    "        # Process melody\n",
    "        melody_sequence = self.process_melody(remi_data['melody'], actual_num_steps)\n",
    "        x_sequence[:actual_num_steps, 0] = melody_sequence\n",
    "\n",
    "        # Process bar\n",
    "        bar_sequence = self.process_bar(remi_data['melody'], actual_num_steps)\n",
    "        x_sequence[:actual_num_steps, 1] = bar_sequence\n",
    "\n",
    "        # Process key\n",
    "        key_sequence = self.process_key(remi_data['key'], actual_num_steps)\n",
    "        x_sequence[:actual_num_steps, 2] = key_sequence\n",
    "\n",
    "        # Process tempo\n",
    "        tempo_sequence = self.process_tempo(remi_data['tempo'], actual_num_steps)\n",
    "        x_sequence[:actual_num_steps, 3] = tempo_sequence\n",
    "\n",
    "        # Process velocity\n",
    "        velocity_sequence = self.process_velocity(remi_data['velocity'], actual_num_steps)\n",
    "        x_sequence[:actual_num_steps, 4] = velocity_sequence\n",
    "\n",
    "        # Process chord\n",
    "        chord_sequence = self.process_chords(remi_data['chords'], actual_num_steps)\n",
    "        y_sequence[:actual_num_steps, 0] = chord_sequence\n",
    "\n",
    "        # Process emotion (if needed)\n",
    "        emotion_sequence = self.process_emotion(remi_data['emotion'], actual_num_steps)\n",
    "        y_sequence[:actual_num_steps, 1] = emotion_sequence\n",
    "\n",
    "        return x_sequence.reshape(1, self.num_steps, self.num_input), y_sequence.reshape(1, self.num_steps, self.num_output)\n",
    "\n",
    "    def get_total_bars(self, melody_events):\n",
    "        total_bars = 0\n",
    "        for event in melody_events:\n",
    "            if event['name'] == 'Bar':\n",
    "                bar_number = int(event['text'])\n",
    "                if bar_number > total_bars:\n",
    "                    total_bars = bar_number\n",
    "        return total_bars\n",
    "\n",
    "    def process_melody(self, melody_events, actual_num_steps):\n",
    "        melody_sequence = [self.vocabs['vocab_note'].token_to_idx['<pad>']] * actual_num_steps\n",
    "        current_note = self.vocabs['vocab_note'].token_to_idx['<pad>']\n",
    "        current_position = None\n",
    "        current_bar = 1\n",
    "\n",
    "        for event in melody_events:\n",
    "            if event['name'] == 'Bar':\n",
    "                current_bar = int(event['text'])\n",
    "            elif event['name'] == 'Position':\n",
    "                pos = float(Fraction(event['value']))\n",
    "                index = int((pos - 1) * 16 + (current_bar - 1) * 16)\n",
    "                current_position = index\n",
    "            elif event['name'] == 'Note On':\n",
    "                pitch = int(event['value'])\n",
    "                current_note = self.vocabs['vocab_note'].token_to_idx.get(str(pitch), self.vocabs['vocab_note'].unk)\n",
    "\n",
    "            # Place the note index at the current position\n",
    "            if current_position is not None and current_position < actual_num_steps:\n",
    "                melody_sequence[current_position] = current_note\n",
    "\n",
    "        # Fill forward\n",
    "        for i in range(1, actual_num_steps):\n",
    "            if melody_sequence[i] == self.vocabs['vocab_note'].token_to_idx['<pad>']:\n",
    "                melody_sequence[i] = melody_sequence[i - 1]\n",
    "\n",
    "        return melody_sequence\n",
    "\n",
    "    def process_bar(self, melody_events, actual_num_steps):\n",
    "        bar_sequence = [self.vocabs['vocab_bar'].token_to_idx['<pad>']] * actual_num_steps\n",
    "        current_bar = 1\n",
    "        for event in melody_events:\n",
    "            if event['name'] == 'Bar':\n",
    "                current_bar = int(event['text'])\n",
    "                # Calculate the start index of the corresponding bar\n",
    "                bar_start_index = (current_bar - 1) * 16\n",
    "                if bar_start_index < actual_num_steps:\n",
    "                    # Fill the current bar number from the start of the current bar to just before the next bar\n",
    "                    end_index = min(bar_start_index + 16, actual_num_steps)\n",
    "                    bar_token = self.vocabs['vocab_bar'].token_to_idx.get(str(current_bar), self.vocabs['vocab_bar'].unk)\n",
    "                    for i in range(bar_start_index, end_index):\n",
    "                        bar_sequence[i] = bar_token\n",
    "        return bar_sequence\n",
    "\n",
    "    def process_key(self, key_events, actual_num_steps):\n",
    "        key_sequence = [self.vocabs['vocab_key'].token_to_idx['<pad>']] * actual_num_steps\n",
    "        current_position = None\n",
    "        current_bar = 1\n",
    "        previous_key = self.vocabs['vocab_key'].token_to_idx['<pad>']\n",
    "\n",
    "        for event in key_events:\n",
    "            if event['name'] == 'Bar':\n",
    "                current_bar = int(event['text'])\n",
    "            elif event['name'] == 'Position':\n",
    "                pos = float(Fraction(event['value']))\n",
    "                index = int((pos - 1) * 16 + (current_bar - 1) * 16)\n",
    "                current_position = index\n",
    "            elif event['name'] == 'Key':\n",
    "                key_value = event['value']\n",
    "                key_token = self.vocabs['vocab_key'].token_to_idx.get(key_value, self.vocabs['vocab_key'].unk)\n",
    "                if current_position is not None and current_position < actual_num_steps:\n",
    "                    key_sequence[current_position] = key_token\n",
    "                    previous_key = key_token\n",
    "\n",
    "        # Fill forward\n",
    "        for i in range(actual_num_steps):\n",
    "            if key_sequence[i] == self.vocabs['vocab_key'].token_to_idx['<pad>']:\n",
    "                key_sequence[i] = previous_key\n",
    "            else:\n",
    "                previous_key = key_sequence[i]\n",
    "\n",
    "        return key_sequence\n",
    "\n",
    "    def process_tempo(self, tempo_events, actual_num_steps):\n",
    "        tempo_sequence = [self.vocabs['vocab_tempo'].token_to_idx['<pad>']] * actual_num_steps\n",
    "        tempo_value = self.vocabs['vocab_tempo'].token_to_idx['<pad>']\n",
    "\n",
    "        if tempo_events:\n",
    "            for event in tempo_events:\n",
    "                if event['name'] == 'Tempo Value':\n",
    "                    tempo_value = self.vocabs['vocab_tempo'].token_to_idx.get(str(event['value']), self.vocabs['vocab_tempo'].unk)\n",
    "                    break  # Use only the first tempo value\n",
    "\n",
    "        tempo_sequence = [tempo_value] * actual_num_steps\n",
    "        return tempo_sequence\n",
    "\n",
    "    def process_velocity(self, velocity_events, actual_num_steps):\n",
    "        velocity_sequence = [self.vocabs['vocab_velocity'].token_to_idx['<pad>']] * actual_num_steps\n",
    "        current_velocity = self.vocabs['vocab_velocity'].token_to_idx['<pad>']\n",
    "        current_position = None\n",
    "        current_bar = 1\n",
    "\n",
    "        for event in velocity_events:\n",
    "            if event['name'] == 'Bar':\n",
    "                current_bar = int(event['text'])\n",
    "            elif event['name'] == 'Position':\n",
    "                pos = float(Fraction(event['value']))\n",
    "                index = int((pos - 1) * 16 + (current_bar - 1) * 16)\n",
    "                current_position = index\n",
    "            elif event['name'] == 'Note Velocity':\n",
    "                velocity_value = event['value']\n",
    "                velocity_token = self.vocabs['vocab_velocity'].token_to_idx.get(str(velocity_value), self.vocabs['vocab_velocity'].unk)\n",
    "                current_velocity = velocity_token\n",
    "\n",
    "            if current_position is not None and current_position < actual_num_steps:\n",
    "                velocity_sequence[current_position] = current_velocity\n",
    "\n",
    "        # Fill forward\n",
    "        for i in range(1, actual_num_steps):\n",
    "            if velocity_sequence[i] == self.vocabs['vocab_velocity'].token_to_idx['<pad>']:\n",
    "                velocity_sequence[i] = velocity_sequence[i - 1]\n",
    "\n",
    "        return velocity_sequence\n",
    "\n",
    "    def process_chords(self, chord_events, actual_num_steps):\n",
    "        chord_sequence = [self.vocabs['vocab_chord'].token_to_idx['<pad>']] * actual_num_steps\n",
    "        current_chord = self.vocabs['vocab_chord'].token_to_idx['<pad>']\n",
    "        current_position = None\n",
    "        current_bar = 1\n",
    "\n",
    "        for event in chord_events:\n",
    "            if event['name'] == 'Bar':\n",
    "                current_bar = int(event['text'])\n",
    "            elif event['name'] == 'Position':\n",
    "                pos = float(Fraction(event['value']))\n",
    "                index = int((pos - 1) * 16 + (current_bar - 1) * 16)\n",
    "                current_position = index\n",
    "            elif event['name'] == 'Chord':\n",
    "                chord_value = event['value']\n",
    "                chord_token = self.vocabs['vocab_chord'].token_to_idx.get(chord_value, self.vocabs['vocab_chord'].unk)\n",
    "                current_chord = chord_token\n",
    "\n",
    "            if current_position is not None and current_position < actual_num_steps:\n",
    "                chord_sequence[current_position] = current_chord\n",
    "\n",
    "        # Fill forward\n",
    "        for i in range(1, actual_num_steps):\n",
    "            if chord_sequence[i] == self.vocabs['vocab_chord'].token_to_idx['<pad>']:\n",
    "                chord_sequence[i] = chord_sequence[i - 1]\n",
    "\n",
    "        return chord_sequence\n",
    "\n",
    "    def process_emotion(self, emotion_events, actual_num_steps):\n",
    "        emotion_sequence = [self.vocabs['vocab_chord'].token_to_idx['<pad>']] * actual_num_steps  # Assuming emotion is second output\n",
    "        emotion_label = self.vocabs['vocab_chord'].token_to_idx['<pad>']\n",
    "\n",
    "        if emotion_events:\n",
    "            for event in emotion_events:\n",
    "                if event['name'] == 'Emotion':\n",
    "                    emotion_label = self.vocabs['vocab_chord'].token_to_idx.get(str(event['value']), self.vocabs['vocab_chord'].unk)\n",
    "                    break  # Use only the first emotion label\n",
    "\n",
    "        emotion_sequence = [emotion_label] * actual_num_steps\n",
    "        return emotion_sequence\n",
    "\n",
    "# Load or Create Vocabularies\n",
    "vocab_path = r\"../vocabs.pkl\"\n",
    "if os.path.exists(vocab_path):\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        vocabs = pickle.load(f)\n",
    "    print(\"Vocabularies loaded.\")\n",
    "else:\n",
    "    # Create lists of tokens from Origin data\n",
    "    note_tokens_list = x_origin[:, :, 0].tolist()\n",
    "    bar_tokens_list = x_origin[:, :, 1].tolist()\n",
    "    key_tokens_list = x_origin[:, :, 2].tolist()\n",
    "    tempo_tokens_list = x_origin[:, :, 3].tolist()\n",
    "    velocity_tokens_list = x_origin[:, :, 4].tolist()\n",
    "    chord_tokens_list = y_origin[:, :, 0].tolist()\n",
    "    \n",
    "    vocabs = {\n",
    "        'vocab_note': Vocab(tokens=note_tokens_list, min_freq=1, reserved_tokens=['<unk>']),\n",
    "        'vocab_bar': Vocab(tokens=bar_tokens_list, min_freq=1, reserved_tokens=['<unk>']),\n",
    "        'vocab_key': Vocab(tokens=key_tokens_list, min_freq=1, reserved_tokens=['<unk>']),\n",
    "        'vocab_tempo': Vocab(tokens=tempo_tokens_list, min_freq=1, reserved_tokens=['<unk>']),\n",
    "        'vocab_velocity': Vocab(tokens=velocity_tokens_list, min_freq=1, reserved_tokens=['<unk>']),\n",
    "        'vocab_chord': Vocab(tokens=chord_tokens_list, min_freq=1, reserved_tokens=['<unk>'])\n",
    "    }\n",
    "    # Save Vocabulary\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump(vocabs, f)\n",
    "    print(\"Vocabularies created and saved.\")\n",
    "\n",
    "print(f\"Vocabulary sizes:\")\n",
    "for vocab_name, vocab in vocabs.items():\n",
    "    print(f\"{vocab_name}: {len(vocab)}\")\n",
    "\n",
    "\n",
    "def remove_key_chord_blocks(input_file_path, output_file_path):\n",
    "    with open(input_file_path, 'r') as infile, open(output_file_path, 'w') as outfile:\n",
    "        lines = infile.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i]\n",
    "            if 'name=Position' in line:\n",
    "                if i + 1 < len(lines):\n",
    "                    next_line = lines[i + 1]\n",
    "                    if 'name=Key' in next_line or 'name=Chord' in next_line:\n",
    "                        i += 2\n",
    "                        continue\n",
    "            if 'name=Key' in line or 'name=Chord' in line:\n",
    "                i += 1\n",
    "                continue\n",
    "            outfile.write(line)\n",
    "            i += 1\n",
    "\n",
    "# generate imperfect REMI without chord & key information. We will convert this imperfect REMI into restored REMI using our model.\n",
    "input_file_path = r'001_remi.txt'\n",
    "output_file_path = r'001_no_key_chord_remi.txt'\n",
    "remove_key_chord_blocks(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f817b1-1592-484c-a7c9-027a04d08e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed\n",
      "Conversion completed\n",
      "REMI to NP conversion completed\n",
      "x_target shape: (1, 1024, 5)\n",
      "y_target shape: (1, 1024, 2)\n",
      "x_source shape: (1, 1024, 5)\n",
      "y_source shape: (1, 1024, 2)\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# part 2. data preprocess\n",
    "# ==========================\n",
    "\n",
    "def indices_to_tokens(vocab, indices):\n",
    "    tokens_list = []\n",
    "    for c in indices:\n",
    "        tokens = vocab.to_tokens(c)\n",
    "        tokens_list.append(tokens)\n",
    "    return tokens_list\n",
    "\n",
    "# IntegratedRemi2Np class instance\n",
    "preprocessor_target = IntegratedRemi2Np(vocabs=vocabs, num_max_bars=64, num_input=5, num_output=2)\n",
    "preprocessor_source = IntegratedRemi2Np(vocabs=vocabs, num_max_bars=64, num_input=5, num_output=2)\n",
    "\n",
    "# REMI files list\n",
    "target_remi_files = [r\"001_remi.txt\"]  # original(with key & chord)\n",
    "source_remi_files = [r\"001_no_key_chord_remi.txt\"]  # source(key & chord removed)\n",
    "\n",
    "# convert to numpy\n",
    "x_target, y_target = preprocessor_target.process_all_files(target_remi_files)\n",
    "x_source, y_source = preprocessor_source.process_all_files(source_remi_files)\n",
    "\n",
    "# save numpy dataset(if needed)\n",
    "np.save(r\"x_target_001.npy\", x_target)\n",
    "np.save(r\"y_target_001.npy\", y_target)\n",
    "\n",
    "np.save(r\"x_source_001.npy\", x_source)\n",
    "np.save(r\"y_source_001.npy\", y_source)\n",
    "\n",
    "print(\"REMI to NP conversion completed\")\n",
    "\n",
    "# data load (if needed)\n",
    "x_target_path = r\"x_target_001.npy\"\n",
    "y_target_path = r\"y_target_001.npy\"\n",
    "\n",
    "x_target = np.load(x_target_path, allow_pickle=True)  # shape: (num_samples, num_steps, 5)\n",
    "y_target = np.load(y_target_path, allow_pickle=True)  # shape: (num_samples, num_steps, 2)\n",
    "\n",
    "print(f\"x_target shape: {x_target.shape}\")\n",
    "print(f\"y_target shape: {y_target.shape}\")\n",
    "\n",
    "x_source_path = r\"x_source_001.npy\"\n",
    "y_source_path = r\"y_source_001.npy\"\n",
    "\n",
    "x_source = np.load(x_source_path, allow_pickle=True)  # shape: (1, num_steps, 5)\n",
    "y_source = np.load(y_source_path, allow_pickle=True)  # shape: (1, num_steps, 2)\n",
    "\n",
    "print(f\"x_source shape: {x_source.shape}\")  # (1, num_steps, 5)\n",
    "print(f\"y_source shape: {y_source.shape}\")  # (1, num_steps, 2)\n",
    "\n",
    "# convert index to token\n",
    "\n",
    "x_target_tokens = {\n",
    "    'note': indices_to_tokens(vocabs['vocab_note'], x_target[:, :, 0]),\n",
    "    'bar': indices_to_tokens(vocabs['vocab_bar'], x_target[:, :, 1]),\n",
    "    'key': indices_to_tokens(vocabs['vocab_key'], x_target[:, :, 2]),\n",
    "    'tempo': indices_to_tokens(vocabs['vocab_tempo'], x_target[:, :, 3]),\n",
    "    'velocity': indices_to_tokens(vocabs['vocab_velocity'], x_target[:, :, 4]),\n",
    "}\n",
    "\n",
    "y_target_tokens = {\n",
    "    'chord': indices_to_tokens(vocabs['vocab_chord'], y_target[:, :, 0]),\n",
    "    'emotion': indices_to_tokens(vocabs['vocab_chord'], y_target[:, :, 1]), \n",
    "}\n",
    "\n",
    "x_source_tokens = {\n",
    "    'note': indices_to_tokens(vocabs['vocab_note'], x_source[:, :, 0]),\n",
    "    'bar': indices_to_tokens(vocabs['vocab_bar'], x_source[:, :, 1]),\n",
    "    'key': indices_to_tokens(vocabs['vocab_key'], x_source[:, :, 2]),\n",
    "    'tempo': indices_to_tokens(vocabs['vocab_tempo'], x_source[:, :, 3]),\n",
    "    'velocity': indices_to_tokens(vocabs['vocab_velocity'], x_source[:, :, 4]),\n",
    "}\n",
    "\n",
    "y_source_tokens = {\n",
    "    'chord': indices_to_tokens(vocabs['vocab_chord'], y_source[:, :, 0]),\n",
    "    'emotion': indices_to_tokens(vocabs['vocab_chord'], y_source[:, :, 1]), \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79792519-9361-4009-846e-65908ed4dca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauoo\\AppData\\Local\\Temp\\ipykernel_25904\\1629374158.py:284: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(deep_bilstm_model_path, map_location=device)\n",
      "C:\\Users\\gauoo\\AppData\\Local\\Temp\\ipykernel_25904\\1629374158.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  integrated_model_loaded.load_state_dict(torch.load(integrated_model_path, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IntegratedModel(\n",
       "  (deep_bilstm_model): DeepBiLSTM(\n",
       "    (embed_note): Embedding(94, 64, padding_idx=0)\n",
       "    (embed_bar): Embedding(66, 16, padding_idx=0)\n",
       "    (lstm): LSTM(80, 256, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout_layer): Dropout(p=0.5, inplace=False)\n",
       "    (fc): Linear(in_features=512, out_features=26, bias=True)\n",
       "  )\n",
       "  (chord_model): Transformer(\n",
       "    (embed_note): Embedding(94, 64, padding_idx=0)\n",
       "    (embed_bar): Embedding(66, 16, padding_idx=0)\n",
       "    (embed_key): Embedding(26, 32, padding_idx=0)\n",
       "    (embed_tempo): Embedding(86, 16, padding_idx=0)\n",
       "    (embed_velocity): Embedding(129, 16, padding_idx=0)\n",
       "    (pos_encoder): PositionalEncoding()\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=144, out_features=144, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=144, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=128, out_features=144, bias=True)\n",
       "          (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=144, out_features=62, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================\n",
    "# Part 3. Model Load\n",
    "# ==========================\n",
    "\n",
    "# Multi-input Process Model\n",
    "class MultiInputModelBase(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_sizes,     \n",
    "        embed_dims,       \n",
    "        hidden_size=256,  \n",
    "        num_layers=3, \n",
    "        num_classes=62,    \n",
    "        dropout=0.5\n",
    "    ):\n",
    "        super(MultiInputModelBase, self).__init__()\n",
    "        # Embedding layers\n",
    "        self.embed_note = nn.Embedding(vocab_sizes['note'], embed_dims['note'], padding_idx=0)\n",
    "        self.embed_bar = nn.Embedding(vocab_sizes['bar'], embed_dims['bar'], padding_idx=0)\n",
    "        self.total_embed_dim = sum(embed_dims.values())\n",
    "\n",
    "# DeepBiLSTM Key Model\n",
    "class DeepBiLSTM(MultiInputModelBase):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_sizes, \n",
    "        embed_dims, \n",
    "        hidden_size=256,   \n",
    "        num_layers=3, \n",
    "        num_classes=62,  \n",
    "        dropout=0.5\n",
    "    ):\n",
    "        super(DeepBiLSTM, self).__init__(vocab_sizes, embed_dims, hidden_size, num_layers, num_classes, dropout)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.total_embed_dim, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Inputs: dictionary of tensors\n",
    "        note = self.embed_note(inputs['note'])       # (batch_size, seq_len, embed_dim)\n",
    "        bar = self.embed_bar(inputs['bar'])\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        x = torch.cat([note, bar], dim=-1)  # (batch_size, seq_len, total_embed_dim)\n",
    "        \n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.dropout_layer(out)\n",
    "        out = self.fc(out)     # (batch_size, seq_len, num_classes)\n",
    "        return out\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1024):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# Transformer Chord Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_sizes, \n",
    "        embed_dims, \n",
    "        num_classes, \n",
    "        nhead=8, \n",
    "        num_encoder_layers=4, \n",
    "        dim_feedforward=512, \n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embed_note = nn.Embedding(vocab_sizes['note'], embed_dims['note'], padding_idx=0)\n",
    "        self.embed_bar = nn.Embedding(vocab_sizes['bar'], embed_dims['bar'], padding_idx=0)\n",
    "        self.embed_key = nn.Embedding(vocab_sizes['key'], embed_dims['key'], padding_idx=0)\n",
    "        self.embed_tempo = nn.Embedding(vocab_sizes['tempo'], embed_dims['tempo'], padding_idx=0)\n",
    "        self.embed_velocity = nn.Embedding(vocab_sizes['velocity'], embed_dims['velocity'], padding_idx=0)\n",
    "        \n",
    "        # Sum of embedding dimensions\n",
    "        self.total_embed_dim = sum(embed_dims.values())\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(self.total_embed_dim)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=self.total_embed_dim, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(self.total_embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, inputs, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (dict): {'note': Tensor, 'bar': Tensor, 'key': Tensor, 'tempo': Tensor, 'velocity': Tensor}\n",
    "            src_key_padding_mask (Tensor, optional): Padding mask (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Chord prediction results (batch_size, seq_len, num_classes)\n",
    "        \"\"\"\n",
    "        note = self.embed_note(inputs['note'])       # (batch_size, seq_len, embed_dim_note)\n",
    "        bar = self.embed_bar(inputs['bar'])          # (batch_size, seq_len, embed_dim_bar)\n",
    "        key = self.embed_key(inputs['key'])          # (batch_size, seq_len, embed_dim_key)\n",
    "        tempo = self.embed_tempo(inputs['tempo'])    # (batch_size, seq_len, embed_dim_tempo)\n",
    "        velocity = self.embed_velocity(inputs['velocity'])  # (batch_size, seq_len, embed_dim_velocity)\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        x = torch.cat([note, bar, key, tempo, velocity], dim=-1)      # (batch_size, seq_len, total_embed_dim)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Pass through Transformer Encoder\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # Final output\n",
    "        x = self.fc(x)                              # (batch_size, seq_len, num_classes)\n",
    "        return x\n",
    "\n",
    "# 4.4 Define Filter Function (No Changes)\n",
    "def apply_window_filter(key_hat, window_size=128):\n",
    "    \"\"\"\n",
    "    key_hat: 1D numpy array of shape (num_steps,)\n",
    "    window_size: int, size of the window (16 or 64 or 128)\n",
    "    \n",
    "    Returns:\n",
    "        filtered_key_hat: 1D numpy array after applying the filter\n",
    "    \"\"\"\n",
    "    key_hat = np.array(key_hat)\n",
    "    output = key_hat.copy()\n",
    "    num_steps = len(key_hat)\n",
    "    num_windows = num_steps // window_size\n",
    "\n",
    "    for n in range(num_windows):\n",
    "        start = n * window_size\n",
    "        end = (n + 1) * window_size\n",
    "        window = key_hat[start:end]\n",
    "        window_non_pad = window[window != 0]\n",
    "\n",
    "        if len(window_non_pad) == 0:\n",
    "            continue  # Skip if only <pad> tokens are present\n",
    "\n",
    "        counts = np.bincount(window_non_pad)\n",
    "        most_freq = np.argmax(counts)\n",
    "        freq = counts[most_freq]\n",
    "\n",
    "        # Check if the most frequent value is unique\n",
    "        if np.sum(counts == freq) > 1:\n",
    "            # If multiple modes exist, try to resolve using the next window's value\n",
    "            replacement = None\n",
    "            for m in range(n + 1, num_windows):\n",
    "                next_start = m * window_size\n",
    "                next_end = (m + 1) * window_size\n",
    "                next_window = key_hat[next_start:end]\n",
    "                next_window_non_pad = next_window[next_window != 0]\n",
    "                if len(next_window_non_pad) == 0:\n",
    "                    continue\n",
    "                next_counts = np.bincount(next_window_non_pad)\n",
    "                next_most_freq = np.argmax(next_counts)\n",
    "                next_freq = next_counts[next_most_freq]\n",
    "                if np.sum(next_counts == next_freq) == 1:\n",
    "                    replacement = next_most_freq\n",
    "                    break\n",
    "            if replacement is not None:\n",
    "                # Replace the current window's non-pad keys with the replacement\n",
    "                window_filtered = np.where(window != 0, replacement, window)\n",
    "                output[start:end] = window_filtered\n",
    "            else:\n",
    "                # If no replacement found, keep the original\n",
    "                continue\n",
    "        else:\n",
    "            # If there is a unique mode, replace with it\n",
    "            replacement = most_freq\n",
    "            window_filtered = np.where(window != 0, replacement, window)\n",
    "            output[start:end] = window_filtered\n",
    "\n",
    "    return output\n",
    "\n",
    "# 4.5 Define IntegratedModel Class (With Changes)\n",
    "class IntegratedModel(nn.Module):\n",
    "    def __init__(self, deep_bilstm_model, chord_model, filter_func, window_size=128):\n",
    "        super(IntegratedModel, self).__init__()\n",
    "        self.deep_bilstm_model = deep_bilstm_model\n",
    "        self.chord_model = chord_model\n",
    "        self.filter_func = filter_func\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Freeze DeepBiLSTM model parameters to prevent training\n",
    "        for param in self.deep_bilstm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (dict): {'note': Tensor, 'bar': Tensor, 'tempo': Tensor, 'velocity': Tensor}\n",
    "\n",
    "        Returns:\n",
    "            outputs_chord (Tensor): Chord prediction results (batch_size, seq_len, num_classes)\n",
    "            predicted_key_tensor (Tensor): Filtered Key prediction results (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        # Predict key using DeepBiLSTM model\n",
    "        with torch.no_grad():\n",
    "            key_inputs = {\n",
    "                'note': inputs['note'],\n",
    "                'bar': inputs['bar']\n",
    "            }\n",
    "            outputs_key = self.deep_bilstm_model(key_inputs)\n",
    "            _, predicted_key = torch.max(outputs_key, dim=2)  # (batch_size, seq_len)\n",
    "            predicted_key = predicted_key.cpu().numpy()\n",
    "\n",
    "            # Apply filter\n",
    "            filtered_predicted_key = []\n",
    "            for pred in predicted_key:\n",
    "                filtered = self.filter_func(pred, window_size=self.window_size)\n",
    "                filtered_predicted_key.append(filtered)\n",
    "\n",
    "            filtered_predicted_key = np.array(filtered_predicted_key)\n",
    "            predicted_key_tensor = torch.from_numpy(filtered_predicted_key).long().to(inputs['note'].device)\n",
    "\n",
    "        # Use the predicted key as input for the Chord prediction model\n",
    "        chord_inputs = {\n",
    "            'note': inputs['note'],\n",
    "            'bar': inputs['bar'],\n",
    "            'key': predicted_key_tensor,\n",
    "            'tempo': inputs['tempo'],\n",
    "            'velocity': inputs['velocity']\n",
    "        }\n",
    "\n",
    "        # Create padding mask (based on note padding)\n",
    "        src_key_padding_mask = (inputs['note'] == 0)  # (batch_size, seq_len)\n",
    "        src_key_padding_mask = src_key_padding_mask.bool()\n",
    "\n",
    "        # Predict chord\n",
    "        outputs_chord = self.chord_model(chord_inputs, src_key_padding_mask=src_key_padding_mask)\n",
    "        return outputs_chord, predicted_key_tensor\n",
    "\n",
    "# Model Loading\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Initialize DeepBiLSTM model\n",
    "deep_bilstm_model_loaded = DeepBiLSTM(\n",
    "    vocab_sizes={'note': len(vocabs['vocab_note']), 'bar': len(vocabs['vocab_bar'])},\n",
    "    embed_dims={'note': 64, 'bar': 16},\n",
    "    hidden_size=256,\n",
    "    num_layers=3,\n",
    "    num_classes=len(vocabs['vocab_key']),\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "deep_bilstm_model_path = r\"../../model_checkpoint/DeepBiLSTM_filtered.pth\" # Modify the path as needed\n",
    "\n",
    "# Modify part: Remove \"model.\" prefix from state_dict keys\n",
    "state_dict = torch.load(deep_bilstm_model_path, map_location=device)\n",
    "\n",
    "# Remove \"model.\" prefix from keys\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('model.'):\n",
    "        new_key = k[6:]  # Remove \"model.\" (first 6 characters)\n",
    "        new_state_dict[new_key] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "# Load the modified state_dict into the model\n",
    "deep_bilstm_model_loaded.load_state_dict(new_state_dict)\n",
    "\n",
    "deep_bilstm_model_loaded.eval()\n",
    "\n",
    "# Initialize Chord Prediction Transformer model\n",
    "chord_model_loaded = Transformer(\n",
    "    vocab_sizes={\n",
    "        'note': len(vocabs['vocab_note']),\n",
    "        'bar': len(vocabs['vocab_bar']),\n",
    "        'key': len(vocabs['vocab_key']),\n",
    "        'tempo': len(vocabs['vocab_tempo']),\n",
    "        'velocity': len(vocabs['vocab_velocity'])\n",
    "    },\n",
    "    embed_dims={\n",
    "        'note': 64,\n",
    "        'bar': 16,\n",
    "        'key': 32,\n",
    "        'tempo': 16,\n",
    "        'velocity': 16\n",
    "    },\n",
    "    num_classes=len(vocabs['vocab_chord']),\n",
    "    nhead=8,\n",
    "    num_encoder_layers=2,\n",
    "    dim_feedforward=128,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# Create IntegratedModel instance\n",
    "integrated_model = IntegratedModel(\n",
    "    deep_bilstm_model=deep_bilstm_model_loaded,\n",
    "    chord_model=chord_model_loaded,\n",
    "    filter_func=apply_window_filter,\n",
    "    window_size=128\n",
    ").to(device)\n",
    "\n",
    "# Duplicate IntegratedModel for loading saved weights\n",
    "integrated_model_loaded = IntegratedModel(\n",
    "    deep_bilstm_model=deep_bilstm_model_loaded,\n",
    "    chord_model=chord_model_loaded,\n",
    "    filter_func=apply_window_filter,\n",
    "    window_size=128\n",
    ").to(device)\n",
    "\n",
    "integrated_model_path = r\"../../model_checkpoint/IntegratedModel_epoch200_valacc0.9303.pt\" # Modify the path as needed\n",
    "integrated_model_loaded.load_state_dict(torch.load(integrated_model_path, map_location=device))\n",
    "integrated_model_loaded.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6dde0-9201-43c6-a606-514e798ceee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Predicted chord & key generated\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================\n",
    "# Part 4. Predict Key & Chord\n",
    "# ==========================\n",
    "\n",
    "class SingleSampleDataset(Dataset):\n",
    "    def __init__(self, x_data):\n",
    "        self.x_data = x_data  # x_data is a numpy array of shape (1, num_steps, 5)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1  # Single sample\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'note': torch.tensor(self.x_data[0, :, 0], dtype=torch.long),\n",
    "            'bar': torch.tensor(self.x_data[0, :, 1], dtype=torch.long),\n",
    "            'tempo': torch.tensor(self.x_data[0, :, 3], dtype=torch.long),\n",
    "            'velocity': torch.tensor(self.x_data[0, :, 4], dtype=torch.long)\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "# Visualization Function\n",
    "def midi_to_note_name(midi_numbers):\n",
    "    \"\"\"\n",
    "    Converts a list of MIDI pitch numbers to a list of note names.\n",
    "\n",
    "    Args:\n",
    "        midi_numbers (list or array): List or array of MIDI pitch numbers\n",
    "\n",
    "    Returns:\n",
    "        note_names (list): List of note names\n",
    "    \"\"\"\n",
    "    note_names_list = ['C', 'C#', 'D', 'D#', 'E', 'F',\n",
    "                       'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "    note_names = []\n",
    "    for num in midi_numbers:\n",
    "        if num == 0:\n",
    "            note_names.append('<pad>')\n",
    "        else:\n",
    "            note_name = note_names_list[int(num) % 12]\n",
    "            note_names.append(note_name)\n",
    "    return note_names\n",
    "\n",
    "\"\"\"\n",
    "Visualizes the model's prediction results for a single sample.\n",
    "\"\"\"\n",
    "\n",
    "# Accuracy Calculation Function\n",
    "def calculate_accuracy(predicted_tokens, target_tokens):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy between two lists of tokens.\n",
    "\n",
    "    Args:\n",
    "        predicted_tokens (list): List of tokens predicted by the model\n",
    "        target_tokens (list): List of target (correct) tokens\n",
    "        \n",
    "    Returns:\n",
    "        float: Accuracy\n",
    "    \"\"\"\n",
    "    total = len(target_tokens)\n",
    "    matches = sum(p == t for p, t in zip(predicted_tokens, target_tokens))\n",
    "    accuracy = matches / total\n",
    "    return accuracy\n",
    "\n",
    "# Initialize the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Create Dataset (pass x_source)\n",
    "single_sample_dataset = SingleSampleDataset(x_source)\n",
    "\n",
    "# Create DataLoader\n",
    "single_sample_loader = DataLoader(single_sample_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "integrated_model_loaded.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in single_sample_loader:\n",
    "        # Move input data to the device\n",
    "        inputs = {\n",
    "            'note': batch['note'].to(device),        # shape: (batch_size, seq_len)\n",
    "            'bar': batch['bar'].to(device),\n",
    "            'tempo': batch['tempo'].to(device),\n",
    "            'velocity': batch['velocity'].to(device)\n",
    "        }\n",
    "\n",
    "        # Perform prediction with the model\n",
    "        outputs_chord, predicted_key_tensor = integrated_model_loaded(inputs)\n",
    "        _, predicted_chord = torch.max(outputs_chord, dim=2)  # predicted_chord shape: (batch_size, seq_len)\n",
    "\n",
    "        # Move predictions to CPU and convert to numpy arrays\n",
    "        predicted_chord_np = predicted_chord.cpu().numpy()        # shape: (batch_size, seq_len)\n",
    "        predicted_key_np = predicted_key_tensor.cpu().numpy()    # shape: (batch_size, seq_len)\n",
    "\n",
    "        print('Predicted chord & key generated')\n",
    "\n",
    "        # Convert predicted key and chord to tokens\n",
    "        predicted_chord_tokens = vocabs['vocab_chord'].to_tokens(predicted_chord_np.squeeze())\n",
    "        predicted_key_tokens = vocabs['vocab_key'].to_tokens(predicted_key_np.squeeze())\n",
    "\n",
    "        # Extract input note sequence (indices)\n",
    "        note_indices_np = batch['note'].cpu().numpy().squeeze()   # shape: (seq_len,)\n",
    "        # Convert indices to tokens (for visualization)\n",
    "        note_tokens = vocabs['vocab_note'].to_tokens(note_indices_np)\n",
    "\n",
    "# Visualization\n",
    "# Sequence length\n",
    "seq_len = len(note_tokens)\n",
    "\n",
    "# Display only a subset of sequences for visualization (e.g., first 64)\n",
    "num_display = min(64, seq_len)\n",
    "x_axis = range(num_display)\n",
    "\n",
    "# Convert note sequence to note names\n",
    "note_names = midi_to_note_name(note_indices_np[:num_display])\n",
    "\n",
    "# Extract actual target tokens\n",
    "targets_chord_tokens = y_target_tokens['chord'][0][:num_display]\n",
    "targets_key_tokens = x_target_tokens['key'][0][:num_display]\n",
    "\n",
    "# Extract predicted tokens (already converted above)\n",
    "predicted_chord_tokens_display = predicted_chord_tokens[:num_display]\n",
    "predicted_key_tokens_display = predicted_key_tokens[:num_display]\n",
    "\n",
    "# Create a set of all tokens\n",
    "all_tokens_set = set(note_names +\n",
    "                     targets_chord_tokens +\n",
    "                     predicted_chord_tokens_display +\n",
    "                     targets_key_tokens +\n",
    "                     predicted_key_tokens_display)\n",
    "\n",
    "# Separate note names and chord names\n",
    "note_names_order = ['C', 'C#', 'D', 'D#', 'E', 'F',\n",
    "                    'F#', 'G', 'G#', 'A', 'A#', 'B', '<pad>']\n",
    "note_names_in_tokens = [name for name in note_names_order if name in all_tokens_set]\n",
    "\n",
    "# Extract chord names (tokens containing ':')\n",
    "chord_names_in_tokens = [token for token in all_tokens_set if ':' in token]\n",
    "chord_names_in_tokens.sort()\n",
    "\n",
    "# Extract key names\n",
    "key_names_in_tokens = [token for token in all_tokens_set if token in vocabs['vocab_key'].idx_to_token]\n",
    "key_names_in_tokens.sort()\n",
    "\n",
    "# Extract other tokens (e.g., '<unk>', etc.)\n",
    "other_tokens = [token for token in all_tokens_set if token not in note_names_in_tokens and\n",
    "                token not in chord_names_in_tokens and token not in key_names_in_tokens]\n",
    "other_tokens.sort()\n",
    "\n",
    "# Set the order of all tokens\n",
    "sorted_all_tokens = note_names_in_tokens + chord_names_in_tokens + key_names_in_tokens + other_tokens\n",
    "\n",
    "# Map tokens to numerical indices\n",
    "token_to_index = {token: idx for idx, token in enumerate(sorted_all_tokens)}\n",
    "index_to_token = {idx: token for token, idx in token_to_index.items()}\n",
    "\n",
    "# Convert note, target, and predicted tokens to numerical indices\n",
    "note_indices_plot = [token_to_index.get(token, -1) for token in note_names[:num_display]]\n",
    "targets_chord_indices_plot = [token_to_index.get(token, -1) for token in targets_chord_tokens]\n",
    "predicted_chord_indices_plot = [token_to_index.get(token, -1) for token in predicted_chord_tokens_display]\n",
    "targets_key_indices_plot = [token_to_index.get(token, -1) for token in targets_key_tokens]\n",
    "predicted_key_indices_plot = [token_to_index.get(token, -1) for token in predicted_key_tokens_display]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.plot(x_axis, note_indices_plot, label='Input Note (x_source)', alpha=0.5)\n",
    "plt.plot(x_axis, targets_chord_indices_plot, label='Target Chord (y_target)', alpha=0.8)\n",
    "plt.plot(x_axis, predicted_chord_indices_plot, label='Predicted Chord', alpha=0.8)\n",
    "plt.plot(x_axis, targets_key_indices_plot, label='Target Key (x_source)', linestyle='-', color='black', linewidth=1)\n",
    "plt.plot(x_axis, predicted_key_indices_plot, label='Predicted Key', linestyle='--', color='gray', linewidth=2)\n",
    "\n",
    "plt.title('Chord and Key Prediction (Integrated Model)')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Token')\n",
    "\n",
    "# Set y-axis ticks to token names\n",
    "plt.yticks(ticks=range(len(sorted_all_tokens)), labels=sorted_all_tokens)\n",
    "\n",
    "plt.legend(loc='upper right')  # Fix legend position to upper right\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'inference_plot.png', dpi=300, bbox_inches='tight')  # Added file extension\n",
    "plt.show()\n",
    "\n",
    "# Calculate Accuracy\n",
    "# Calculate accuracy between two token lists\n",
    "def calculate_accuracy(predicted_tokens, target_tokens):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy between two lists of tokens.\n",
    "    \n",
    "    Args:\n",
    "        predicted_tokens (list): List of tokens predicted by the model\n",
    "        target_tokens (list): List of target (correct) tokens\n",
    "        \n",
    "    Returns:\n",
    "        float: Accuracy\n",
    "    \"\"\"\n",
    "    total = len(target_tokens)\n",
    "    matches = sum(p == t for p, t in zip(predicted_tokens, target_tokens))\n",
    "    accuracy = matches / total\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy for Chord and Key\n",
    "chord_accuracy = calculate_accuracy(predicted_chord_tokens_display, targets_chord_tokens)\n",
    "key_accuracy = calculate_accuracy(predicted_key_tokens_display, targets_key_tokens)\n",
    "\n",
    "# Print results\n",
    "print(f\"Predicted Chord vs Target Chord Accuracy: {chord_accuracy:.4f}\")\n",
    "print(f\"Predicted Key vs Target Key Accuracy: {key_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a17f01-d122-46cb-80c6-5207dae5e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fractions import Fraction\n",
    "from collections import defaultdict\n",
    "\n",
    "# ==========================\n",
    "# Part 5. REMI Restoration\n",
    "# ==========================\n",
    "\n",
    "# Set file paths\n",
    "input_remi_file = r\"001_no_key_chord_remi.txt\"\n",
    "output_remi_file = r\"001_processed_remi.txt\"\n",
    "\n",
    "\n",
    "def parse_remi_file_blocks(filepath):\n",
    "    \"\"\"\n",
    "    Parses a REMI file into event blocks and returns a list of blocks.\n",
    "    Each block consists of a list of event dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the REMI file\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: List of (block_type, block_events) tuples\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    current_block = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        num_lines = len(lines)\n",
    "        i = 0\n",
    "        while i < num_lines:\n",
    "            line = lines[i].strip()\n",
    "            if not line:\n",
    "                i += 1\n",
    "                continue  # Skip empty lines\n",
    "            event = parse_event_line(line)\n",
    "            if not event:\n",
    "                i += 1\n",
    "                continue  # Skip invalid events\n",
    "\n",
    "            if event['name'] == 'Note On':\n",
    "                # Note block includes the two previous events and the next event\n",
    "                note_block = []\n",
    "                if i >= 2:\n",
    "                    prev_event1 = parse_event_line(lines[i - 2].strip())\n",
    "                    prev_event2 = parse_event_line(lines[i - 1].strip())\n",
    "                    if prev_event1 and prev_event2 and prev_event1['name'] == 'Position' and prev_event2['name'] == 'Note Velocity':\n",
    "                        note_block.extend([prev_event1, prev_event2])\n",
    "                note_block.append(event)  # 'Note On' event\n",
    "                # Add the next event ('Note Duration')\n",
    "                if i + 1 < num_lines:\n",
    "                    next_event = parse_event_line(lines[i + 1].strip())\n",
    "                    if next_event and next_event['name'] == 'Note Duration':\n",
    "                        note_block.append(next_event)\n",
    "                        i += 1  # Increment index as 'Note Duration' has been processed\n",
    "                blocks.append(('note', note_block))\n",
    "            elif event['name'] == 'Bar':\n",
    "                # Bar block consists of a single event\n",
    "                blocks.append(('bar', [event]))\n",
    "            elif event['name'] == 'Tempo Class':\n",
    "                # Tempo block includes the current event and the next event\n",
    "                tempo_block = [event]\n",
    "                if i + 1 < num_lines:\n",
    "                    next_event = parse_event_line(lines[i + 1].strip())\n",
    "                    if next_event and next_event['name'] == 'Tempo Value':\n",
    "                        tempo_block.append(next_event)\n",
    "                        i += 1  # Increment index as 'Tempo Value' has been processed\n",
    "                blocks.append(('tempo', tempo_block))\n",
    "            else:\n",
    "                # Skip other events\n",
    "                pass\n",
    "            i += 1\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def parse_event_line(line):\n",
    "    \"\"\"\n",
    "    Parses a single line from a REMI file and returns an event dictionary.\n",
    "    \n",
    "    Args:\n",
    "        line (str): A single line from the REMI file\n",
    "    \n",
    "    Returns:\n",
    "        dict: Event dictionary\n",
    "    \"\"\"\n",
    "    if not line.startswith('Event('):\n",
    "        return None\n",
    "    event = {}\n",
    "    # Extract name\n",
    "    if \"name=\" in line:\n",
    "        event_name = line.split(\"name=\")[1].split(\",\")[0].strip()\n",
    "        event['name'] = event_name\n",
    "    else:\n",
    "        return None  # Skip lines without 'name='\n",
    "\n",
    "    # Extract time\n",
    "    if \"time=\" in line:\n",
    "        event_time = line.split(\"time=\")[1].split(\",\")[0].strip()\n",
    "        event['time'] = int(event_time)\n",
    "    else:\n",
    "        event['time'] = None  # Or set to 0\n",
    "\n",
    "    # Extract value\n",
    "    if \"value=\" in line:\n",
    "        event_value = line.split(\"value=\")[1].split(\",\")[0].strip()\n",
    "        event['value'] = event_value\n",
    "    else:\n",
    "        event['value'] = None\n",
    "\n",
    "    # Extract text\n",
    "    if \"text=\" in line:\n",
    "        event_text = line.split(\"text=\")[1].strip().rstrip(')')\n",
    "        event['text'] = event_text\n",
    "    else:\n",
    "        event['text'] = None\n",
    "\n",
    "    return event\n",
    "\n",
    "\n",
    "# Define ticks\n",
    "ticks_per_bar = 960  # 960 ticks per bar\n",
    "ticks_per_time_step = ticks_per_bar / 16  # 60 ticks per time step\n",
    "\n",
    "\n",
    "def time_step_to_tick_time(t0, time_step):\n",
    "    \"\"\"\n",
    "    Converts a time step to tick time.\n",
    "    \n",
    "    Args:\n",
    "        t0 (int): Initial time (ticks)\n",
    "        time_step (int): Time step index\n",
    "    \n",
    "    Returns:\n",
    "        int: Tick time\n",
    "    \"\"\"\n",
    "    tick_time = t0 + ticks_per_time_step * time_step\n",
    "    return int(tick_time)\n",
    "\n",
    "\n",
    "def get_initial_time(note_dict, bar_dict):\n",
    "    \"\"\"\n",
    "    Returns the initial time based on the first 'Note' block.\n",
    "    \n",
    "    Args:\n",
    "        bar_dict (dict): Dictionary of Bar blocks\n",
    "        note_dict (dict): Dictionary of Note blocks\n",
    "    \n",
    "    Returns:\n",
    "        int: Initial time (ticks)\n",
    "    \"\"\"\n",
    "    if note_dict:\n",
    "        return min(note_dict.keys()) + min(bar_dict.keys())\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_last_time(blocks):\n",
    "    \"\"\"\n",
    "    Returns the time of the last event in the song.\n",
    "    \n",
    "    Args:\n",
    "        blocks (list): List of (block_type, block_events) tuples\n",
    "    \n",
    "    Returns:\n",
    "        int: Time of the last event (ticks)\n",
    "    \"\"\"\n",
    "    last_time = 0\n",
    "    for _, block in blocks:\n",
    "        for event in block:\n",
    "            if event['time'] is not None and event['time'] > last_time:\n",
    "                last_time = event['time']\n",
    "    return last_time\n",
    "\n",
    "\n",
    "def get_num_time_steps(t0, last_time, ticks_per_time_step):\n",
    "    \"\"\"\n",
    "    Calculates the number of time steps based on the last time.\n",
    "    \n",
    "    Args:\n",
    "        t0 (int): Initial time\n",
    "        last_time (int): Last time in the song\n",
    "        ticks_per_time_step (float): Ticks per time step\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of time steps in the song\n",
    "    \"\"\"\n",
    "    total_ticks = last_time - t0\n",
    "    num_time_steps = int(total_ticks / ticks_per_time_step) + 1\n",
    "    return num_time_steps\n",
    "\n",
    "\n",
    "def find_change_points(tokens):\n",
    "    \"\"\"\n",
    "    Finds change points in a token sequence and returns their indices and tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): Token sequence\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: List of (time_step_index, token) tuples\n",
    "    \"\"\"\n",
    "    change_points = []\n",
    "    prev_token = None\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token != prev_token:\n",
    "            change_points.append((i, token))\n",
    "            prev_token = token\n",
    "    return change_points\n",
    "\n",
    "# Parse REMI file\n",
    "blocks = parse_remi_file_blocks(input_remi_file)\n",
    "print(f\"Total number of blocks in the original REMI file: {len(blocks)}\")\n",
    "\n",
    "\n",
    "# Store Blocks in Dictionaries\n",
    "note_dict = defaultdict(list)\n",
    "bar_dict = {}\n",
    "tempo_dict = {}\n",
    "# Chord and Key are generated from model predictions, so they are excluded here\n",
    "\n",
    "for block_type, block in blocks:\n",
    "    time = block[0]['time']  # All events within a block have the same time\n",
    "\n",
    "    if block_type == 'note':\n",
    "        note_dict[time].append(block)\n",
    "    elif block_type == 'bar':\n",
    "        bar_dict[time] = block\n",
    "    elif block_type == 'tempo':\n",
    "        tempo_dict[time] = block\n",
    "\n",
    "print(f\"Number of Note blocks: {sum(len(v) for v in note_dict.values())}\")\n",
    "print(f\"Number of Bar blocks: {len(bar_dict)}\")\n",
    "print(f\"Number of Tempo blocks: {len(tempo_dict)}\")\n",
    "\n",
    "# Define necessary variables and functions\n",
    "\n",
    "# Initial time (t0) extraction\n",
    "def get_initial_time(note_dict, bar_dict):\n",
    "    \"\"\"\n",
    "    Returns the initial time based on the first 'Note' block.\n",
    "    \n",
    "    Args:\n",
    "        bar_dict (dict): Dictionary of Bar blocks\n",
    "        note_dict (dict): Dictionary of Note blocks\n",
    "    \n",
    "    Returns:\n",
    "        int: Initial time (ticks)\n",
    "    \"\"\"\n",
    "    if note_dict:\n",
    "        return min(note_dict.keys()) + min(bar_dict.keys())\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "t0 = get_initial_time(note_dict, bar_dict)\n",
    "print(f\"Initial time (t0): {t0}\")\n",
    "\n",
    "# Extract the last time of the song\n",
    "last_time_in_song = get_last_time(blocks)\n",
    "print(f\"Last time in the song: {last_time_in_song}\")\n",
    "\n",
    "# Calculate the number of time steps in the song\n",
    "num_time_steps_in_song = get_num_time_steps(t0, last_time_in_song, ticks_per_time_step)\n",
    "print(f\"Number of time steps in the song: {num_time_steps_in_song}\")\n",
    "\n",
    "# Trim prediction results to match the song length\n",
    "predicted_chord_tokens = predicted_chord_tokens[:num_time_steps_in_song]\n",
    "predicted_key_tokens = predicted_key_tokens[:num_time_steps_in_song]\n",
    "\n",
    "# Find change points\n",
    "chord_change_points = find_change_points(predicted_chord_tokens)\n",
    "key_change_points = find_change_points(predicted_key_tokens)\n",
    "\n",
    "print(f\"Chord change points: {chord_change_points}\")\n",
    "print(f\"Key change points: {key_change_points}\")\n",
    "\n",
    "# Create Chord and Key dictionaries\n",
    "chord_dict = {}\n",
    "key_dict = {}\n",
    "\n",
    "# Create Chord dictionary\n",
    "for time_step, chord_token in chord_change_points:\n",
    "    tick_time = time_step_to_tick_time(t0, time_step)\n",
    "    position_in_bar = (time_step % 16) + 1  # From 1 to 16\n",
    "    position_value = f\"{position_in_bar}/16\"\n",
    "\n",
    "    # Create Position event\n",
    "    position_event = {\n",
    "        'name': 'Position',\n",
    "        'time': tick_time,\n",
    "        'value': position_value,\n",
    "        'text': str(tick_time)\n",
    "    }\n",
    "    # Create Chord event\n",
    "    chord_event = {\n",
    "        'name': 'Chord',\n",
    "        'time': tick_time,\n",
    "        'value': chord_token,\n",
    "        'text': chord_token\n",
    "    }\n",
    "    # Create block\n",
    "    chord_block = [position_event, chord_event]\n",
    "    chord_dict.setdefault(tick_time, []).append(chord_block)\n",
    "\n",
    "# Create Key dictionary\n",
    "for time_step, key_token in key_change_points:\n",
    "    tick_time = time_step_to_tick_time(t0, time_step)\n",
    "    position_in_bar = (time_step % 16) + 1  # From 1 to 16\n",
    "    position_value = f\"{position_in_bar}/16\"\n",
    "\n",
    "    # Create Position event\n",
    "    position_event = {\n",
    "        'name': 'Position',\n",
    "        'time': tick_time,\n",
    "        'value': position_value,\n",
    "        'text': str(tick_time)\n",
    "    }\n",
    "    # Create Key event\n",
    "    key_event = {\n",
    "        'name': 'Key',\n",
    "        'time': tick_time,\n",
    "        'value': key_token,\n",
    "        'text': key_token\n",
    "    }\n",
    "    # Create block\n",
    "    key_block = [position_event, key_event]\n",
    "    key_dict.setdefault(tick_time, []).append(key_block)\n",
    "\n",
    "print(f\"Number of Chord blocks created: {sum(len(v) for v in chord_dict.values())}\")\n",
    "print(f\"Number of Key blocks created: {sum(len(v) for v in key_dict.values())}\")\n",
    "\n",
    "# Combine dictionaries\n",
    "from collections import defaultdict\n",
    "\n",
    "# Collect all time keys\n",
    "all_times = set()\n",
    "all_times.update(tempo_dict.keys())\n",
    "all_times.update(bar_dict.keys())\n",
    "all_times.update(key_dict.keys())\n",
    "all_times.update(chord_dict.keys())\n",
    "all_times.update(note_dict.keys())\n",
    "\n",
    "# Define block priority: tempo > bar > key > chord > note\n",
    "block_priority = {\n",
    "    'tempo': 0,\n",
    "    'bar': 1,\n",
    "    'key': 2,\n",
    "    'chord': 3,\n",
    "    'note': 4,\n",
    "}\n",
    "\n",
    "# Create combined_dict\n",
    "combined_dict = defaultdict(list)\n",
    "\n",
    "for time in sorted(all_times):\n",
    "    blocks_at_time = []\n",
    "\n",
    "    # Add blocks based on priority\n",
    "    if time in tempo_dict:\n",
    "        blocks_at_time.append(('tempo', tempo_dict[time]))\n",
    "    if time in bar_dict:\n",
    "        blocks_at_time.append(('bar', bar_dict[time]))\n",
    "    if time in key_dict:\n",
    "        for key_block in key_dict[time]:\n",
    "            blocks_at_time.append(('key', key_block))\n",
    "    if time in chord_dict:\n",
    "        for chord_block in chord_dict[time]:\n",
    "            blocks_at_time.append(('chord', chord_block))\n",
    "    if time in note_dict:\n",
    "        # Add all note blocks\n",
    "        for note_block in note_dict[time]:\n",
    "            blocks_at_time.append(('note', note_block))\n",
    "\n",
    "    # Sort blocks based on priority\n",
    "    blocks_at_time.sort(key=lambda x: block_priority.get(x[0], 100))\n",
    "\n",
    "    # Add to combined_dict\n",
    "    for block_type, block in blocks_at_time:\n",
    "        combined_dict[time].append(block)\n",
    "\n",
    "print(f\"Total number of combined time keys: {len(combined_dict)}\")\n",
    "\n",
    "\n",
    "# Write to REMI file\n",
    "with open(output_remi_file, 'w') as output_file:\n",
    "    # Save combined_dict contents in chronological order\n",
    "    for time in sorted(combined_dict.keys()):\n",
    "        blocks = combined_dict[time]\n",
    "        for block in blocks:\n",
    "            for event in block:\n",
    "                event_str = f\"Event(name={event['name']}, time={event['time']}, value={event['value']}, text={event['text']})\"\n",
    "                output_file.write(event_str + '\\n')\n",
    "            # output_file.write('\\n')  # Add empty line to separate blocks\n",
    "\n",
    "print(f\"A new REMI file has been created: {output_remi_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55e90f-60f8-4a3f-9a91-f8a475223ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
