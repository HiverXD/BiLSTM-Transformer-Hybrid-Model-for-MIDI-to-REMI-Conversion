{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1467b281-194a-467b-97e0-f17e24416842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import collections\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# ==========================\n",
    "# Vocab Class Definition\n",
    "# ==========================\n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text tokens.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        # Create list of unique tokens\n",
    "        self.idx_to_token = ['<pad>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq and token != '<pad>'\n",
    "        ]\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self.token_to_idx.get('<unk>', 1)  # Returns index 1 if '<unk>' is not in the vocabulary\n",
    "\n",
    "# ==========================\n",
    "# Window Filter Function Definition\n",
    "# ==========================\n",
    "def apply_window_filter(key_hat, window_size=128):\n",
    "    \"\"\"\n",
    "    Applies a window filter to the predicted key sequence.\n",
    "    \n",
    "    Args:\n",
    "        key_hat (1D numpy array): Array of predicted key indices.\n",
    "        window_size (int): Size of the window (e.g., 64, 128).\n",
    "    \n",
    "    Returns:\n",
    "        filtered_key_hat (1D numpy array): Filtered key sequence.\n",
    "    \"\"\"\n",
    "    key_hat = np.array(key_hat)\n",
    "    output = key_hat.copy()\n",
    "    num_steps = len(key_hat)\n",
    "    num_windows = num_steps // window_size\n",
    "\n",
    "    for n in range(num_windows):\n",
    "        start = n * window_size\n",
    "        end = (n + 1) * window_size\n",
    "        window = key_hat[start:end]\n",
    "        window_non_pad = window[window != 0]\n",
    "\n",
    "        if len(window_non_pad) == 0:\n",
    "            continue  # Skip if only <pad> exists in the window\n",
    "\n",
    "        counts = np.bincount(window_non_pad)\n",
    "        most_freq = np.argmax(counts)\n",
    "        freq = counts[most_freq]\n",
    "\n",
    "        # Check if the most frequent value is unique\n",
    "        if np.sum(counts == freq) > 1:\n",
    "            # If multiple values have the same highest frequency, attempt to resolve using the next window\n",
    "            replacement = None\n",
    "            for m in range(n + 1, num_windows):\n",
    "                next_start = m * window_size\n",
    "                next_end = (m + 1) * window_size\n",
    "                next_window = key_hat[next_start:next_end]\n",
    "                next_window_non_pad = next_window[next_window != 0]\n",
    "                if len(next_window_non_pad) == 0:\n",
    "                    continue\n",
    "                next_counts = np.bincount(next_window_non_pad)\n",
    "                next_most_freq = np.argmax(next_counts)\n",
    "                next_freq = next_counts[next_most_freq]\n",
    "                if np.sum(next_counts == next_freq) == 1:\n",
    "                    replacement = next_most_freq\n",
    "                    break\n",
    "            if replacement is not None:\n",
    "                # Replace non-pad keys in the current window with the replacement\n",
    "                window_filtered = np.where(window != 0, replacement, window)\n",
    "                output[start:end] = window_filtered\n",
    "            else:\n",
    "                # If replacement cannot be determined, retain the original window\n",
    "                continue\n",
    "        else:\n",
    "            # Replace non-pad keys in the window with the most frequent key\n",
    "            replacement = most_freq\n",
    "            window_filtered = np.where(window != 0, replacement, window)\n",
    "            output[start:end] = window_filtered\n",
    "\n",
    "    return output\n",
    "\n",
    "# ==========================\n",
    "# Dataset Class Definition\n",
    "# ==========================\n",
    "class MultiInputDataset(Dataset):\n",
    "    def __init__(self, note, bar, key, tempo, velocity, chord):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with note, bar, key, tempo, velocity, and chord tensors.\n",
    "        \n",
    "        Args:\n",
    "            note (Tensor): Tensor of shape (num_samples, num_steps)\n",
    "            bar (Tensor): Tensor of shape (num_samples, num_steps)\n",
    "            key (Tensor): Tensor of shape (num_samples, num_steps)\n",
    "            tempo (Tensor): Tensor of shape (num_samples, num_steps)\n",
    "            velocity (Tensor): Tensor of shape (num_samples, num_steps)\n",
    "            chord (Tensor): Tensor of shape (num_samples, num_steps)\n",
    "        \"\"\"\n",
    "        self.note = note\n",
    "        self.bar = bar\n",
    "        self.key = key\n",
    "        self.tempo = tempo\n",
    "        self.velocity = velocity\n",
    "        self.chord = chord\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.note.size(0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'note': self.note[idx],\n",
    "            'bar': self.bar[idx],\n",
    "            'key': self.key[idx],\n",
    "            'tempo': self.tempo[idx],\n",
    "            'velocity': self.velocity[idx],\n",
    "            'chord': self.chord[idx]\n",
    "        }\n",
    "\n",
    "# ==========================\n",
    "# Base Model Class Definition\n",
    "# ==========================\n",
    "class MultiInputModelBase(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_sizes,       # Dictionary of vocab sizes for each feature\n",
    "        embed_dims,        # Dictionary of embedding dimensions for each feature\n",
    "        hidden_size=256,   # Hidden state size\n",
    "        num_layers=3, \n",
    "        num_classes=62,    # Number of target classes (key_vocab_size)\n",
    "        dropout=0.5\n",
    "    ):\n",
    "        super(MultiInputModelBase, self).__init__()\n",
    "        # Embedding layers (using note and bar only)\n",
    "        self.embed_note = nn.Embedding(vocab_sizes['note'], embed_dims['note'], padding_idx=0)\n",
    "        self.embed_bar = nn.Embedding(vocab_sizes['bar'], embed_dims['bar'], padding_idx=0)\n",
    "        \n",
    "        # Sum of embedding dimensions\n",
    "        self.total_embed_dim = sum(embed_dims.values())\n",
    "\n",
    "# ==========================\n",
    "# DeepBiLSTM Model Definition\n",
    "# ==========================\n",
    "class DeepBiLSTM(MultiInputModelBase):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_sizes, \n",
    "        embed_dims, \n",
    "        hidden_size=256,    # Hidden state size\n",
    "        num_layers=3, \n",
    "        num_classes=62,     # Number of target classes (key_vocab_size)\n",
    "        dropout=0.5\n",
    "    ):\n",
    "        super(DeepBiLSTM, self).__init__(vocab_sizes, embed_dims, hidden_size, num_layers, num_classes, dropout)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.total_embed_dim, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # inputs: dictionary of tensors\n",
    "        note = self.embed_note(inputs['note'])       # (batch_size, seq_len, embed_dim)\n",
    "        bar = self.embed_bar(inputs['bar'])\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        x = torch.cat([note, bar], dim=-1)  # (batch_size, seq_len, total_embed_dim)\n",
    "        \n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.dropout_layer(out)\n",
    "        out = self.fc(out)     # (batch_size, seq_len, num_classes)\n",
    "        return out\n",
    "\n",
    "# ==========================\n",
    "# Positional Encoding for Transformer\n",
    "# ==========================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1024):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# ==========================\n",
    "# DenoiseTransformer Model Definition\n",
    "# ==========================\n",
    "class DenoiseTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_sizes, \n",
    "        embed_dims, \n",
    "        num_classes, \n",
    "        nhead=8, \n",
    "        num_encoder_layers=4, \n",
    "        dim_feedforward=512, \n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super(DenoiseTransformer, self).__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embed_note = nn.Embedding(vocab_sizes['note'], embed_dims['note'], padding_idx=0)\n",
    "        self.embed_bar = nn.Embedding(vocab_sizes['bar'], embed_dims['bar'], padding_idx=0)\n",
    "        self.embed_key = nn.Embedding(vocab_sizes['key'], embed_dims['key'], padding_idx=0)\n",
    "        self.embed_tempo = nn.Embedding(vocab_sizes['tempo'], embed_dims['tempo'], padding_idx=0)\n",
    "        self.embed_velocity = nn.Embedding(vocab_sizes['velocity'], embed_dims['velocity'], padding_idx=0)\n",
    "        \n",
    "        # Sum of embedding dimensions\n",
    "        self.total_embed_dim = sum(embed_dims.values())\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(self.total_embed_dim)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=self.total_embed_dim, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # Output Layer\n",
    "        self.fc = nn.Linear(self.total_embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, inputs, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (dict): {\n",
    "                'note': Tensor, \n",
    "                'bar': Tensor, \n",
    "                'key': Tensor, \n",
    "                'tempo': Tensor, \n",
    "                'velocity': Tensor\n",
    "            }\n",
    "            src_key_padding_mask (Tensor, optional): Padding mask (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Chord prediction results (batch_size, seq_len, num_classes)\n",
    "        \"\"\"\n",
    "        note = self.embed_note(inputs['note'])       # (batch_size, seq_len, embed_dim_note)\n",
    "        bar = self.embed_bar(inputs['bar'])          # (batch_size, seq_len, embed_dim_bar)\n",
    "        key = self.embed_key(inputs['key'])          # (batch_size, seq_len, embed_dim_key)\n",
    "        tempo = self.embed_tempo(inputs['tempo'])    # (batch_size, seq_len, embed_dim_tempo)\n",
    "        velocity = self.embed_velocity(inputs['velocity'])  # (batch_size, seq_len, embed_dim_velocity)\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        x = torch.cat([note, bar, key, tempo, velocity], dim=-1)      # (batch_size, seq_len, total_embed_dim)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Pass through Transformer Encoder\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # Final output\n",
    "        x = self.fc(x)                              # (batch_size, seq_len, num_classes)\n",
    "        return x\n",
    "\n",
    "# ==========================\n",
    "# IntegratedModel Class Definition\n",
    "# ==========================\n",
    "class IntegratedModel(nn.Module):\n",
    "    def __init__(self, deep_bilstm_model, chord_model, filter_func, window_size=128):\n",
    "        \"\"\"\n",
    "        Initializes the IntegratedModel with DeepBiLSTM and DenoiseTransformer models.\n",
    "        \n",
    "        Args:\n",
    "            deep_bilstm_model (DeepBiLSTM): Pretrained DeepBiLSTM model for key prediction.\n",
    "            chord_model (DenoiseTransformer): Pretrained DenoiseTransformer model for chord prediction.\n",
    "            filter_func (function): Function to apply filtering on key predictions.\n",
    "            window_size (int): Window size for filtering.\n",
    "        \"\"\"\n",
    "        super(IntegratedModel, self).__init__()\n",
    "        self.deep_bilstm_model = deep_bilstm_model\n",
    "        self.chord_model = chord_model\n",
    "        self.filter_func = filter_func\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Freeze DeepBiLSTM model parameters\n",
    "        for param in self.deep_bilstm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the IntegratedModel.\n",
    "        \n",
    "        Args:\n",
    "            inputs (dict): {\n",
    "                'note': Tensor, \n",
    "                'bar': Tensor, \n",
    "                'tempo': Tensor, \n",
    "                'velocity': Tensor\n",
    "            }\n",
    "        \n",
    "        Returns:\n",
    "            outputs_chord (Tensor): Chord prediction results (batch_size, seq_len, num_classes)\n",
    "            predicted_key_tensor (Tensor): Filtered Key prediction results (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        # Predict keys using DeepBiLSTM model\n",
    "        with torch.no_grad():\n",
    "            key_inputs = {\n",
    "                'note': inputs['note'],\n",
    "                'bar': inputs['bar']\n",
    "            }\n",
    "            outputs_key = self.deep_bilstm_model(key_inputs)  # (batch_size, seq_len, num_classes)\n",
    "            _, predicted_key = torch.max(outputs_key, dim=2)  # (batch_size, seq_len)\n",
    "            predicted_key = predicted_key.cpu().numpy()\n",
    "\n",
    "            # Apply filter\n",
    "            filtered_predicted_key = []\n",
    "            for pred in predicted_key:\n",
    "                filtered = self.filter_func(pred, window_size=self.window_size)\n",
    "                filtered_predicted_key.append(filtered)\n",
    "\n",
    "            filtered_predicted_key = np.array(filtered_predicted_key)\n",
    "            predicted_key_tensor = torch.from_numpy(filtered_predicted_key).long().to(inputs['note'].device)\n",
    "\n",
    "        # Prepare inputs for Chord prediction model\n",
    "        chord_inputs = {\n",
    "            'note': inputs['note'],\n",
    "            'bar': inputs['bar'],\n",
    "            'key': predicted_key_tensor,\n",
    "            'tempo': inputs['tempo'],\n",
    "            'velocity': inputs['velocity']\n",
    "        }\n",
    "\n",
    "        # Create padding mask based on 'note' inputs\n",
    "        src_key_padding_mask = (inputs['note'] == 0)  # (batch_size, seq_len)\n",
    "        src_key_padding_mask = src_key_padding_mask.bool()\n",
    "\n",
    "        # Predict chords\n",
    "        outputs_chord = self.chord_model(chord_inputs, src_key_padding_mask=src_key_padding_mask)  # (batch_size, seq_len, num_classes)\n",
    "        return outputs_chord, predicted_key_tensor\n",
    "\n",
    "# ==========================\n",
    "# Model Saving and Loading Functions\n",
    "# ==========================\n",
    "def save_models_and_vocabs(models, vocabs, save_dir):\n",
    "    \"\"\"\n",
    "    Saves multiple Vocab objects and models to the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        models (dict): Dictionary with model names as keys and model instances as values.\n",
    "        vocabs (dict): Dictionary with vocab names as keys and Vocab objects as values.\n",
    "        save_dir (str): Directory path to save the models and Vocab objects.\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        print(f\"Directory created: {save_dir}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {save_dir}\")\n",
    "    \n",
    "    # Save Vocab objects using pickle\n",
    "    vocab_save_path = os.path.join(save_dir, 'vocabs.pkl')\n",
    "    with open(vocab_save_path, 'wb') as f:\n",
    "        pickle.dump(vocabs, f)\n",
    "        print(f\"Vocab objects saved: {vocab_save_path}\")\n",
    "    \n",
    "    # Save each model\n",
    "    for name, model in models.items():\n",
    "        save_path = os.path.join(save_dir, f\"{name}.pth\")  # e.g., DeepBiLSTM.pth\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Model '{name}' saved: {save_path}\")\n",
    "    \n",
    "    print(\"All Vocab objects and models have been successfully saved.\")\n",
    "\n",
    "def load_models_and_vocabs(model_classes, save_dir, device):\n",
    "    \"\"\"\n",
    "    Loads saved Vocab objects and multiple models from the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        model_classes (dict): Dictionary with model names as keys and model classes as values.\n",
    "        save_dir (str): Directory path where models and Vocab objects are saved.\n",
    "        device (torch.device): Device to load the models onto (CPU or GPU).\n",
    "    \n",
    "    Returns:\n",
    "        loaded_models (dict): Dictionary with model names as keys and loaded model instances as values.\n",
    "        vocabs (dict): Dictionary with vocab names as keys and loaded Vocab objects as values.\n",
    "    \"\"\"\n",
    "    # Load Vocab objects using pickle\n",
    "    vocab_save_path = os.path.join(save_dir, 'vocabs.pkl')\n",
    "    with open(vocab_save_path, 'rb') as f:\n",
    "        vocabs = pickle.load(f)\n",
    "        print(\"Vocab objects loaded.\")\n",
    "    \n",
    "    # Initialize a dictionary to store loaded models\n",
    "    loaded_models = {}\n",
    "    \n",
    "    # Load each model\n",
    "    for name, model_class in model_classes.items():\n",
    "        # Instantiate the model\n",
    "        if name == 'DeepBiLSTM':\n",
    "            model = model_class(\n",
    "                vocab_sizes={'note': len(vocabs['vocab_note']), 'bar': len(vocabs['vocab_bar'])},\n",
    "                embed_dims={'note': 64, 'bar': 16},\n",
    "                hidden_size=256,\n",
    "                num_layers=3,\n",
    "                num_classes=len(vocabs['vocab_key']),\n",
    "                dropout=0.5\n",
    "            )\n",
    "        elif name == 'DenoiseTransformer':\n",
    "            model = model_class(\n",
    "                vocab_sizes={\n",
    "                    'note': len(vocabs['vocab_note']),\n",
    "                    'bar': len(vocabs['vocab_bar']),\n",
    "                    'key': len(vocabs['vocab_key']),\n",
    "                    'tempo': len(vocabs['vocab_tempo']),\n",
    "                    'velocity': len(vocabs['vocab_velocity'])\n",
    "                },\n",
    "                embed_dims={\n",
    "                    'note': 64,\n",
    "                    'bar': 16,\n",
    "                    'key': 32,\n",
    "                    'tempo': 16,\n",
    "                    'velocity': 16\n",
    "                },\n",
    "                num_classes=len(vocabs['vocab_chord']),\n",
    "                nhead=8,\n",
    "                num_encoder_layers=4,\n",
    "                dim_feedforward=512,\n",
    "                dropout=0.1\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model name: {name}\")\n",
    "        \n",
    "        # Move model to the specified device\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load the model's state_dict\n",
    "        load_path = os.path.join(save_dir, f\"{name}.pth\")\n",
    "        if os.path.exists(load_path):\n",
    "            model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "            print(f\"Model '{name}' loaded from {load_path}\")\n",
    "        else:\n",
    "            print(f\"Model file for '{name}' does not exist at {load_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Add the loaded model to the dictionary\n",
    "        loaded_models[name] = model\n",
    "    \n",
    "    print(\"All models and Vocab objects have been successfully loaded.\")\n",
    "    return loaded_models, vocabs\n",
    "\n",
    "# ==========================\n",
    "# Evaluation and Visualization Functions\n",
    "# ==========================\n",
    "def compute_validation_accuracy(models, valid_loader, device, filter_func, window_size=64):\n",
    "    \"\"\"\n",
    "    Computes the validation accuracy for each model after applying a filter.\n",
    "    \n",
    "    Args:\n",
    "        models (dict): Dictionary with model names as keys and loaded model instances as values.\n",
    "        valid_loader (DataLoader): Validation DataLoader.\n",
    "        device (torch.device): Device to perform computations on.\n",
    "        filter_func (function): Function to apply filtering on key predictions.\n",
    "        window_size (int): Window size for filtering.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy_dict (dict): Dictionary with model names as keys and accuracy scores as values.\n",
    "    \"\"\"\n",
    "    accuracy_dict = {name: {'correct': 0, 'total': 0} for name in models.keys()}\n",
    "    \n",
    "    for batch in valid_loader:\n",
    "        note = batch['note'].to(device)\n",
    "        bar = batch['bar'].to(device)\n",
    "        tempo = batch['tempo'].to(device)\n",
    "        velocity = batch['velocity'].to(device)\n",
    "        key = batch['key'].to(device)\n",
    "        chord = batch['chord'].to(device)\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                if name == 'IntegratedModel':\n",
    "                    # Assuming IntegratedModel handles its own forwarding\n",
    "                    outputs_chord, predicted_key = model({\n",
    "                        'note': note,\n",
    "                        'bar': bar,\n",
    "                        'tempo': tempo,\n",
    "                        'velocity': velocity\n",
    "                    })\n",
    "                    predicted_key = predicted_key.cpu().numpy()\n",
    "                else:\n",
    "                    # For individual models, handle inputs accordingly\n",
    "                    if name == 'DeepBiLSTM':\n",
    "                        key_inputs = {\n",
    "                            'note': note,\n",
    "                            'bar': bar\n",
    "                        }\n",
    "                        outputs_key = model(key_inputs)\n",
    "                        _, predicted_key = torch.max(outputs_key, dim=2)\n",
    "                        predicted_key = predicted_key.cpu().numpy()\n",
    "                    elif name == 'DenoiseTransformer':\n",
    "                        chord_inputs = {\n",
    "                            'note': note,\n",
    "                            'bar': bar,\n",
    "                            'key': key,\n",
    "                            'tempo': tempo,\n",
    "                            'velocity': velocity\n",
    "                        }\n",
    "                        outputs_chord = model(chord_inputs)\n",
    "                        _, predicted_chord = torch.max(outputs_chord, dim=2)\n",
    "                        predicted_chord = predicted_chord.cpu().numpy()\n",
    "                        predicted_key = key.cpu().numpy()  # Use ground truth key for accuracy\n",
    "                    else:\n",
    "                        continue  # Skip unknown models\n",
    "                \n",
    "                # Apply filter to predicted keys\n",
    "                if name == 'IntegratedModel':\n",
    "                    filtered_predicted_key = np.array([filter_func(pred, window_size) for pred in predicted_key])\n",
    "                    filtered_predicted_key = torch.from_numpy(filtered_predicted_key).long().to(device)\n",
    "                else:\n",
    "                    filtered_predicted_key = np.array([filter_func(pred, window_size) for pred in predicted_key])\n",
    "                    filtered_predicted_key = torch.from_numpy(filtered_predicted_key).long().to(device)\n",
    "                \n",
    "                # Compute accuracy excluding <pad> tokens\n",
    "                if name == 'IntegratedModel':\n",
    "                    # Use predicted_chord for IntegratedModel\n",
    "                    correct = ((outputs_chord.argmax(dim=2) == chord) & (chord != 0)).sum().item()\n",
    "                    total = (chord != 0).sum().item()\n",
    "                elif name == 'DenoiseTransformer':\n",
    "                    correct = ((predicted_chord == chord) & (chord != 0)).sum().item()\n",
    "                    total = (chord != 0).sum().item()\n",
    "                else:\n",
    "                    correct = ((filtered_predicted_key == key) & (key != 0)).sum().item()\n",
    "                    total = (key != 0).sum().item()\n",
    "                \n",
    "                accuracy_dict[name]['correct'] += correct\n",
    "                accuracy_dict[name]['total'] += total\n",
    "    \n",
    "    # Calculate final accuracy scores\n",
    "    for name in accuracy_dict:\n",
    "        if accuracy_dict[name]['total'] > 0:\n",
    "            accuracy_dict[name] = accuracy_dict[name]['correct'] / accuracy_dict[name]['total']\n",
    "        else:\n",
    "            accuracy_dict[name] = 0.0\n",
    "    \n",
    "    return accuracy_dict\n",
    "\n",
    "def midi_to_note_name(midi_numbers):\n",
    "    \"\"\"\n",
    "    Converts a list or array of MIDI pitch numbers to a list of note names.\n",
    "    \n",
    "    Args:\n",
    "        midi_numbers (list or array): List or array of MIDI pitch numbers\n",
    "    \n",
    "    Returns:\n",
    "        note_names (list): List of note names\n",
    "    \"\"\"\n",
    "    note_names_list = ['C', 'C#', 'D', 'D#', 'E', 'F',\n",
    "                       'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "    note_names = []\n",
    "    for num in midi_numbers:\n",
    "        if num == 0:\n",
    "            note_names.append('<pad>')\n",
    "        else:\n",
    "            note_name = note_names_list[int(num) % 12]\n",
    "            note_names.append(note_name)\n",
    "    return note_names\n",
    "\n",
    "def visualize_sample_prediction(models, sample, sample_index, vocabs, start_step=0, end_step=64):\n",
    "    \"\"\"\n",
    "    Visualizes the model's prediction results for a specific sample in the validation dataset and saves it as a PNG file.\n",
    "\n",
    "    Args:\n",
    "        models (dict): Dictionary with model names as keys and loaded model instances as values.\n",
    "        sample (dict): Sample from the validation dataset.\n",
    "        sample_index (int): Index of the sample in the validation dataset.\n",
    "        vocabs (dict): Dictionary with vocab names as keys and Vocab objects as values.\n",
    "        start_step (int): Starting time step to visualize (inclusive).\n",
    "        end_step (int): Ending time step to visualize (exclusive).\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    inputs = {\n",
    "        'note': sample['note'].unsqueeze(0).to(device),\n",
    "        'bar': sample['bar'].unsqueeze(0).to(device),\n",
    "        'tempo': sample['tempo'].unsqueeze(0).to(device),\n",
    "        'velocity': sample['velocity'].unsqueeze(0).to(device)\n",
    "    }\n",
    "    targets_chord = sample['chord'].unsqueeze(0).to(device)\n",
    "    targets_key = sample['key'].unsqueeze(0).to(device)\n",
    "\n",
    "    # Get predictions from each model\n",
    "    predictions = {}\n",
    "    with torch.no_grad():\n",
    "        for name, model in models.items():\n",
    "            if name == 'IntegratedModel':\n",
    "                outputs_chord, predicted_key = model(inputs)\n",
    "                _, predicted_chord = torch.max(outputs_chord, dim=2)\n",
    "                predictions[name] = {\n",
    "                    'predicted_chord': predicted_chord.cpu().numpy().squeeze(),\n",
    "                    'predicted_key': predicted_key.cpu().numpy().squeeze()\n",
    "                }\n",
    "            elif name == 'DeepBiLSTM':\n",
    "                key_inputs = {\n",
    "                    'note': inputs['note'],\n",
    "                    'bar': inputs['bar']\n",
    "                }\n",
    "                outputs_key = model(key_inputs)\n",
    "                _, predicted_key = torch.max(outputs_key, dim=2)\n",
    "                predictions[name] = {\n",
    "                    'predicted_key': predicted_key.cpu().numpy().squeeze()\n",
    "                }\n",
    "            elif name == 'DenoiseTransformer':\n",
    "                chord_inputs = {\n",
    "                    'note': inputs['note'],\n",
    "                    'bar': inputs['bar'],\n",
    "                    'key': targets_key,\n",
    "                    'tempo': inputs['tempo'],\n",
    "                    'velocity': inputs['velocity']\n",
    "                }\n",
    "                outputs_chord = model(chord_inputs)\n",
    "                _, predicted_chord = torch.max(outputs_chord, dim=2)\n",
    "                predictions[name] = {\n",
    "                    'predicted_chord': predicted_chord.cpu().numpy().squeeze()\n",
    "                }\n",
    "\n",
    "    # Convert tokens to strings\n",
    "    targets_chord_tokens = vocabs['vocab_chord'].to_tokens(targets_chord.cpu().numpy().squeeze())\n",
    "    targets_key_tokens = vocabs['vocab_key'].to_tokens(targets_key.cpu().numpy().squeeze())\n",
    "    note_tokens_indices = inputs['note'].cpu().numpy().squeeze()\n",
    "    note_tokens_sample = midi_to_note_name(note_tokens_indices)\n",
    "\n",
    "    # Plotting\n",
    "    seq_length = len(targets_chord_tokens)\n",
    "    start_step = max(0, start_step)\n",
    "    end_step = min(end_step, seq_length)\n",
    "\n",
    "    if start_step >= end_step:\n",
    "        print(f\"Start time step {start_step} is greater than or equal to the sequence length {seq_length}. Skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    x = range(start_step, end_step)\n",
    "\n",
    "    # Generate set of all tokens within the specified range\n",
    "    all_tokens_set = set(note_tokens_sample[start_step:end_step] + \n",
    "                         targets_chord_tokens[start_step:end_step] + \n",
    "                         targets_key_tokens[start_step:end_step])\n",
    "    for pred in predictions.values():\n",
    "        if 'predicted_chord' in pred:\n",
    "            all_tokens_set.update(pred['predicted_chord'][start_step:end_step])\n",
    "        if 'predicted_key' in pred:\n",
    "            all_tokens_set.update(pred['predicted_key'][start_step:end_step])\n",
    "\n",
    "    # Separate note names, chord names, and key names\n",
    "    note_names_order = ['C', 'C#', 'D', 'D#', 'E', 'F',\n",
    "                        'F#', 'G', 'G#', 'A', 'A#', 'B', '<pad>']\n",
    "    note_names_in_tokens = [name for name in note_names_order if name in all_tokens_set]\n",
    "    \n",
    "    # Chord names (assuming chords contain ':', e.g., \"C:maj\")\n",
    "    chord_names_in_tokens = [token for token in all_tokens_set if ':' in token]\n",
    "    chord_names_in_tokens.sort()\n",
    "    \n",
    "    # Key names\n",
    "    key_names_in_tokens = [token for token in all_tokens_set if token in vocabs['vocab_key'].idx_to_token]\n",
    "    key_names_in_tokens.sort()\n",
    "    \n",
    "    # Other tokens\n",
    "    other_tokens = [token for token in all_tokens_set if token not in note_names_in_tokens and \n",
    "                    token not in chord_names_in_tokens and token not in key_names_in_tokens]\n",
    "    other_tokens.sort()\n",
    "    \n",
    "    # Set the order of all tokens\n",
    "    sorted_all_tokens = note_names_in_tokens + chord_names_in_tokens + key_names_in_tokens + other_tokens\n",
    "    \n",
    "    # Map tokens to numeric indices\n",
    "    token_to_index = {token: idx for idx, token in enumerate(sorted_all_tokens)}\n",
    "    \n",
    "    # Convert tokens to numeric indices\n",
    "    note_indices_plot = [token_to_index.get(token, -1) for token in note_tokens_sample[start_step:end_step]]\n",
    "    targets_chord_indices_plot = [token_to_index.get(token, -1) for token in targets_chord_tokens[start_step:end_step]]\n",
    "    targets_key_indices_plot = [token_to_index.get(token, -1) for token in targets_key_tokens[start_step:end_step]]\n",
    "    \n",
    "    # Prepare predicted indices\n",
    "    predicted_chord_indices_plot = {}\n",
    "    predicted_key_indices_plot = {}\n",
    "    for name, pred in predictions.items():\n",
    "        if 'predicted_chord' in pred:\n",
    "            predicted_chord_indices_plot[name] = [token_to_index.get(token, -1) for token in pred['predicted_chord'][start_step:end_step]]\n",
    "        if 'predicted_key' in pred:\n",
    "            predicted_key_indices_plot[name] = [token_to_index.get(token, -1) for token in pred['predicted_key'][start_step:end_step]]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Plot Input Note\n",
    "    plt.plot(x, note_indices_plot, label='Input Note', alpha=0.5, color='blue')\n",
    "    \n",
    "    # Plot Target Chord\n",
    "    plt.plot(x, targets_chord_indices_plot, label='Target Chord', alpha=0.8, color='green')\n",
    "    \n",
    "    # Plot Predicted Chord\n",
    "    for name, indices in predicted_chord_indices_plot.items():\n",
    "        plt.plot(x, indices, label=f'Predicted Chord ({name})', linestyle='--')\n",
    "    \n",
    "    # Plot Target Key\n",
    "    plt.plot(x, targets_key_indices_plot, label='Target Key', linestyle='-', color='black', linewidth=2)\n",
    "    \n",
    "    # Plot Predicted Key\n",
    "    for name, indices in predicted_key_indices_plot.items():\n",
    "        plt.plot(x, indices, label=f'Predicted Key ({name})', linestyle='--', linewidth=2)\n",
    "    \n",
    "    plt.title(f'Chord and Key Prediction (Sample {sample_index})')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Token')\n",
    "    \n",
    "    # Set y-axis ticks to token names\n",
    "    plt.yticks(ticks=range(len(sorted_all_tokens)), labels=sorted_all_tokens)\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Set directory to save the graph\n",
    "    save_dir_visual = 'results'\n",
    "    if not os.path.exists(save_dir_visual):\n",
    "        os.makedirs(save_dir_visual)\n",
    "    \n",
    "    # Save the graph as PNG (including time step range in the filename)\n",
    "    save_path = os.path.join(save_dir_visual, f'pred_result_{sample_index}_{start_step}_{end_step}.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Prediction result saved to {save_path}\")\n",
    "    \n",
    "    # Display the graph\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7746a57-3262-4b9d-87bb-076da985f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import collections\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# ==========================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ==========================\n",
    "# Define data paths\n",
    "x_path = r\"aug_x_pop.npy\"\n",
    "y_path = r\"aug_y_pop.npy\"\n",
    "\n",
    "# Load data\n",
    "x = np.load(x_path, allow_pickle=True)  # shape: (num_samples, num_steps, 5)\n",
    "y = np.load(y_path, allow_pickle=True)  # shape: (num_samples, num_steps, 2)\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Separate features from x\n",
    "note_tokens = x[:, :, 0]      # (num_samples, num_steps)\n",
    "bar_tokens = x[:, :, 1]       # (num_samples, num_steps)\n",
    "key_tokens = x[:, :, 2]       # (num_samples, num_steps) # Actual key (ground truth)\n",
    "tempo_tokens = x[:, :, 3]     # (num_samples, num_steps)\n",
    "velocity_tokens = x[:, :, 4]  # (num_samples, num_steps)\n",
    "\n",
    "# Separate feature from y (using only chord)\n",
    "chord_tokens = y[:, :, 0]     # (num_samples, num_steps)\n",
    "\n",
    "# Create token lists for each feature\n",
    "note_tokens_list = note_tokens.tolist()\n",
    "bar_tokens_list = bar_tokens.tolist()\n",
    "key_tokens_list = key_tokens.tolist()\n",
    "tempo_tokens_list = tempo_tokens.tolist()\n",
    "velocity_tokens_list = velocity_tokens.tolist()\n",
    "chord_tokens_list = chord_tokens.tolist()\n",
    "\n",
    "# Load or create vocabularies (vocabs.pkl)\n",
    "vocab_path = r\"vocabs.pkl\"\n",
    "if os.path.exists(vocab_path):\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        vocabs = pickle.load(f)\n",
    "    print(\"Vocabularies loaded.\")\n",
    "else:\n",
    "    vocabs = {\n",
    "        'vocab_note': Vocab(tokens=note_tokens_list, min_freq=1, reserved_tokens=['<unk>']),\n",
    "        'vocab_bar': Vocab(tokens=bar_tokens_list, min_freq=1, reserved_tokens=['<unk>']),\n",
    "        'vocab_key': Vocab(tokens=key_tokens_list, min_freq=1, reserved_tokens=['<unk>']),\n",
    "        'vocab_tempo': Vocab(tokens=tempo_tokens_list, min_freq=1, reserved_tokens=['<unk>']),\n",
    "        'vocab_velocity': Vocab(tokens=velocity_tokens_list, min_freq=1, reserved_tokens=['<unk>']),\n",
    "        'vocab_chord': Vocab(tokens=chord_tokens_list, min_freq=1, reserved_tokens=['<unk>'])\n",
    "    }\n",
    "    # Save vocabularies\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump(vocabs, f)\n",
    "    print(\"Vocabularies created and saved.\")\n",
    "\n",
    "print(f\"Vocabulary sizes:\")\n",
    "for vocab_name, vocab in vocabs.items():\n",
    "    print(f\"{vocab_name}: {len(vocab)}\")\n",
    "\n",
    "# Convert tokens to indices\n",
    "note_indices = [vocabs['vocab_note'][line] for line in note_tokens_list]\n",
    "bar_indices = [vocabs['vocab_bar'][line] for line in bar_tokens_list]\n",
    "key_indices = [vocabs['vocab_key'][line] for line in key_tokens_list]\n",
    "tempo_indices = [vocabs['vocab_tempo'][line] for line in tempo_tokens_list]\n",
    "velocity_indices = [vocabs['vocab_velocity'][line] for line in velocity_tokens_list]\n",
    "chord_indices = [vocabs['vocab_chord'][line] for line in chord_tokens_list]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "note_indices = np.array(note_indices)\n",
    "bar_indices = np.array(bar_indices)\n",
    "key_indices = np.array(key_indices)\n",
    "tempo_indices = np.array(tempo_indices)\n",
    "velocity_indices = np.array(velocity_indices)\n",
    "chord_indices = np.array(chord_indices)\n",
    "\n",
    "# Convert to Tensors\n",
    "note_tensor = torch.from_numpy(note_indices).long()         # (num_samples, num_steps)\n",
    "bar_tensor = torch.from_numpy(bar_indices).long()           # (num_samples, num_steps)\n",
    "key_tensor = torch.from_numpy(key_indices).long()           # (num_samples, num_steps)\n",
    "tempo_tensor = torch.from_numpy(tempo_indices).long()       # (num_samples, num_steps)\n",
    "velocity_tensor = torch.from_numpy(velocity_indices).long() # (num_samples, num_steps)\n",
    "chord_tensor = torch.from_numpy(chord_indices).long()       # (num_samples, num_steps)\n",
    "\n",
    "# ==========================\n",
    "# 2. Dataset and DataLoader Preparation\n",
    "# ==========================\n",
    "# Create the dataset (including key_tensor)\n",
    "dataset = MultiInputDataset(\n",
    "    note_tensor, bar_tensor, key_tensor, tempo_tensor, velocity_tensor, chord_tensor\n",
    ")\n",
    "\n",
    "# Split the dataset (80% training, 20% validation)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Valid dataset size: {len(valid_dataset)}\")\n",
    "\n",
    "# Define collate_fn function\n",
    "def collate_fn(batch):\n",
    "    notes = [item['note'] for item in batch]\n",
    "    bars = [item['bar'] for item in batch]\n",
    "    keys = [item['key'] for item in batch]\n",
    "    tempos = [item['tempo'] for item in batch]\n",
    "    velocities = [item['velocity'] for item in batch]\n",
    "    chords = [item['chord'] for item in batch]\n",
    "\n",
    "    seq_lengths = [len(seq) for seq in notes]\n",
    "    max_length = max(seq_lengths)\n",
    "\n",
    "    padded_notes = torch.zeros(len(batch), max_length, dtype=torch.long)\n",
    "    padded_bars = torch.zeros(len(batch), max_length, dtype=torch.long)\n",
    "    padded_keys = torch.zeros(len(batch), max_length, dtype=torch.long)\n",
    "    padded_tempos = torch.zeros(len(batch), max_length, dtype=torch.long)\n",
    "    padded_velocities = torch.zeros(len(batch), max_length, dtype=torch.long)\n",
    "    padded_chords = torch.zeros(len(batch), max_length, dtype=torch.long)\n",
    "\n",
    "    for i, length in enumerate(seq_lengths):\n",
    "        padded_notes[i, :length] = batch[i]['note'][:length]\n",
    "        padded_bars[i, :length] = batch[i]['bar'][:length]\n",
    "        padded_keys[i, :length] = batch[i]['key'][:length]\n",
    "        padded_tempos[i, :length] = batch[i]['tempo'][:length]\n",
    "        padded_velocities[i, :length] = batch[i]['velocity'][:length]\n",
    "        padded_chords[i, :length] = batch[i]['chord'][:length]\n",
    "\n",
    "    return {\n",
    "        'note': padded_notes,\n",
    "        'bar': padded_bars,\n",
    "        'key': padded_keys,\n",
    "        'tempo': padded_tempos,\n",
    "        'velocity': padded_velocities,\n",
    "        'chord': padded_chords\n",
    "    }\n",
    "\n",
    "# Define DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# ==========================\n",
    "# 3. Model Initialization and Loading\n",
    "# ==========================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Initialize DeepBiLSTM model\n",
    "deep_bilstm_model_loaded = DeepBiLSTM(\n",
    "    vocab_sizes={'note': len(vocabs['vocab_note']), 'bar': len(vocabs['vocab_bar'])},\n",
    "    embed_dims={'note': 64, 'bar': 16},\n",
    "    hidden_size=256,\n",
    "    num_layers=3,\n",
    "    num_classes=len(vocabs['vocab_key']),\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "deep_bilstm_model_path = r\"DeepBiLSTM_filtered.pth\"\n",
    "\n",
    "# Load state_dict (remove \"model.\" prefix if present)\n",
    "state_dict = torch.load(deep_bilstm_model_path, map_location=device)\n",
    "\n",
    "# Remove \"model.\" prefix from keys\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('model.'):\n",
    "        new_key = k[6:]  # Remove \"model.\" (first 6 characters)\n",
    "        new_state_dict[new_key] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "# Load the modified state_dict into the model\n",
    "deep_bilstm_model_loaded.load_state_dict(new_state_dict)\n",
    "\n",
    "deep_bilstm_model_loaded.eval()\n",
    "\n",
    "# Initialize Chord prediction model\n",
    "chord_model_loaded = DenoiseTransformer(\n",
    "    vocab_sizes={\n",
    "        'note': len(vocabs['vocab_note']),\n",
    "        'bar': len(vocabs['vocab_bar']),\n",
    "        'key': len(vocabs['vocab_key']),\n",
    "        'tempo': len(vocabs['vocab_tempo']),\n",
    "        'velocity': len(vocabs['vocab_velocity'])\n",
    "    },\n",
    "    embed_dims={\n",
    "        'note': 64,\n",
    "        'bar': 16,\n",
    "        'key': 32,\n",
    "        'tempo': 16,\n",
    "        'velocity': 16\n",
    "    },\n",
    "    num_classes=len(vocabs['vocab_chord']),\n",
    "    nhead=8,\n",
    "    num_encoder_layers=4,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Initialize IntegratedModel\n",
    "integrated_model = IntegratedModel(\n",
    "    deep_bilstm_model=deep_bilstm_model_loaded,\n",
    "    chord_model=chord_model_loaded,\n",
    "    filter_func=apply_window_filter,\n",
    "    window_size=128\n",
    ").to(device)\n",
    "\n",
    "# ==========================\n",
    "# 4. Loss Function and Optimizer Definition\n",
    "# ==========================\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, integrated_model.parameters()), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Initialize history dictionary to record loss and accuracy\n",
    "history = {'train_loss': [], 'valid_loss': [], 'train_acc': [], 'valid_acc': []}\n",
    "\n",
    "# Set directory to save models\n",
    "save_dir = r\"integrated_model\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    print(f\"Directory created: {save_dir}\")\n",
    "\n",
    "# Save Vocabulary\n",
    "vocab_save_path = os.path.join(save_dir, 'vocabs.pkl')\n",
    "with open(vocab_save_path, 'wb') as f:\n",
    "    pickle.dump(vocabs, f)\n",
    "    print(f\"Vocabularies saved at {vocab_save_path}\")\n",
    "\n",
    "# ==========================\n",
    "# 5. Suppress Warnings (Optional)\n",
    "# ==========================\n",
    "warnings.filterwarnings(\"ignore\", message=\"Torch was not compiled with flash attention.\")\n",
    "\n",
    "# ==========================\n",
    "# 6. Training and Validation Loop\n",
    "# ==========================\n",
    "num_epochs = 100\n",
    "best_valid_acc = 0.0  # Initialize best validation accuracy\n",
    "start_time = time.time()  # Record training start time\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_start_time = time.time()  # Record epoch start time\n",
    "    print(f'\\nEpoch {epoch}/{num_epochs}')\n",
    "\n",
    "    # ======================\n",
    "    # Training Phase\n",
    "    # ======================\n",
    "    integrated_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in train_loader:\n",
    "        # Extract inputs\n",
    "        inputs = {\n",
    "            'note': batch['note'].to(device),\n",
    "            'bar': batch['bar'].to(device),\n",
    "            'tempo': batch['tempo'].to(device),\n",
    "            'velocity': batch['velocity'].to(device)\n",
    "        }\n",
    "        targets = batch['chord'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs_chord, predicted_key = integrated_model(inputs)  # (batch_size, seq_len, num_classes), (batch_size, seq_len)\n",
    "\n",
    "        # Reshape for loss: (batch_size * seq_len, num_classes)\n",
    "        outputs_reshaped = outputs_chord.view(-1, len(vocabs['vocab_chord']))\n",
    "        targets_reshaped = targets.view(-1)  # (batch_size * seq_len)\n",
    "\n",
    "        loss = criterion(outputs_reshaped, targets_reshaped)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * targets.size(0)\n",
    "\n",
    "        # Compute predictions\n",
    "        _, predicted = torch.max(outputs_chord, dim=2)  # (batch_size, seq_len)\n",
    "        correct += ((predicted == targets) & (targets != 0)).sum().item()  # Exclude <pad> tokens\n",
    "        total += (targets != 0).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / train_size\n",
    "    epoch_acc = correct / total\n",
    "    history['train_loss'].append(epoch_loss)\n",
    "    history['train_acc'].append(epoch_acc)\n",
    "\n",
    "    # ======================\n",
    "    # Validation Phase\n",
    "    # ======================\n",
    "    integrated_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            inputs = {\n",
    "                'note': batch['note'].to(device),\n",
    "                'bar': batch['bar'].to(device),\n",
    "                'tempo': batch['tempo'].to(device),\n",
    "                'velocity': batch['velocity'].to(device)\n",
    "            }\n",
    "            targets = batch['chord'].to(device)\n",
    "\n",
    "            outputs_chord, predicted_key = integrated_model(inputs)\n",
    "\n",
    "            outputs_reshaped = outputs_chord.view(-1, len(vocabs['vocab_chord']))\n",
    "            targets_reshaped = targets.view(-1)\n",
    "\n",
    "            loss = criterion(outputs_reshaped, targets_reshaped)\n",
    "            val_loss += loss.item() * targets.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs_chord, dim=2)\n",
    "            val_correct += ((predicted == targets) & (targets != 0)).sum().item()\n",
    "            val_total += (targets != 0).sum().item()\n",
    "\n",
    "    epoch_val_loss = val_loss / valid_size\n",
    "    epoch_val_acc = val_correct / val_total\n",
    "    history['valid_loss'].append(epoch_val_loss)\n",
    "    history['valid_acc'].append(epoch_val_acc)\n",
    "\n",
    "    # ======================\n",
    "    # Periodic Output\n",
    "    # ======================\n",
    "    if epoch % 10 == 0 or epoch == 1 or epoch == num_epochs:\n",
    "        print(f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, '\n",
    "              f'Valid Loss: {epoch_val_loss:.4f}, Valid Acc: {epoch_val_acc:.4f}')\n",
    "\n",
    "    # ======================\n",
    "    # Save Model if Validation Accuracy Improved\n",
    "    # ======================\n",
    "    if epoch % 2 == 0 and epoch_val_acc > best_valid_acc:\n",
    "        best_valid_acc = epoch_val_acc\n",
    "        model_save_path = os.path.join(save_dir, f'IntegratedModel_epoch{epoch}_valacc{epoch_val_acc:.4f}.pt')\n",
    "        torch.save(integrated_model.state_dict(), model_save_path)\n",
    "        print(f\"Integrated Model saved at epoch {epoch} with validation accuracy {epoch_val_acc:.4f}\")\n",
    "\n",
    "    # ======================\n",
    "    # Calculate Elapsed and Remaining Time\n",
    "    # ======================\n",
    "    epoch_end_time = time.time()\n",
    "    elapsed_time = epoch_end_time - start_time\n",
    "    remaining_time = elapsed_time * (num_epochs / epoch) - elapsed_time\n",
    "\n",
    "    # Print time information\n",
    "    print(f'Epoch {epoch}/{num_epochs} completed. Time elapsed: {elapsed_time:.2f} seconds. '\n",
    "          f'Estimated remaining time: {remaining_time:.2f} seconds.')\n",
    "\n",
    "# ==========================\n",
    "# 7. Loss and Accuracy Visualization\n",
    "# ==========================\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(1, num_epochs + 1), history['train_loss'], label='Train Loss', linestyle='-')\n",
    "plt.plot(range(1, num_epochs + 1), history['valid_loss'], label='Valid Loss', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(1, num_epochs + 1), history['train_acc'], label='Train Acc', linestyle='-')\n",
    "plt.plot(range(1, num_epochs + 1), history['valid_acc'], label='Valid Acc', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the loss and accuracy graphs\n",
    "results_dir = 'results'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "save_path = os.path.join(results_dir, 'tuned-model_train_graph.png')\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ==========================\n",
    "# 8. Load Best Model and Visualize Predictions\n",
    "# ==========================\n",
    "# Function to find best model files based on validation accuracy\n",
    "def find_best_model_files(save_dir):\n",
    "    model_files = glob.glob(os.path.join(save_dir, '*.pt'))\n",
    "    best_models = {}\n",
    "    pattern = r'(.+)_epoch(\\d+)_valacc([\\d.]+)\\.pt'\n",
    "    for file in model_files:\n",
    "        filename = os.path.basename(file)\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            model_name, epoch, valacc = match.groups()\n",
    "            valacc = float(valacc)\n",
    "            if model_name not in best_models or valacc > best_models[model_name]['valacc']:\n",
    "                best_models[model_name] = {'file': file, 'epoch': int(epoch), 'valacc': valacc}\n",
    "    return best_models\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {\n",
    "                'note': batch['note'].to(device),\n",
    "                'bar': batch['bar'].to(device),\n",
    "                'tempo': batch['tempo'].to(device),\n",
    "                'velocity': batch['velocity'].to(device)\n",
    "            }\n",
    "            targets = batch['chord'].to(device)\n",
    "\n",
    "            outputs_chord, predicted_key = model(inputs)  # (batch_size, seq_len, num_classes), (batch_size, seq_len)\n",
    "            outputs_reshaped = outputs_chord.view(-1, len(vocabs['vocab_chord']))\n",
    "            targets_reshaped = targets.view(-1)\n",
    "\n",
    "            loss = criterion(outputs_reshaped, targets_reshaped)\n",
    "            val_loss += loss.item() * targets.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs_chord, dim=2)\n",
    "            val_correct += ((predicted == targets) & (targets != 0)).sum().item()\n",
    "            val_total += (targets != 0).sum().item()\n",
    "\n",
    "    avg_loss = val_loss / len(dataloader.dataset)\n",
    "    avg_acc = val_correct / val_total if val_total > 0 else 0\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "# Find best model files\n",
    "best_models = find_best_model_files(save_dir)\n",
    "\n",
    "# Initialize a dictionary to store evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "# Evaluate each best model\n",
    "for model_name, info in best_models.items():\n",
    "    print(f\"Evaluating best model for {model_name}: {info['file']} (Epoch {info['epoch']}, Val Acc {info['valacc']:.4f})\")\n",
    "    \n",
    "    # Check if the model name exists in the mapping\n",
    "    if model_name == 'IntegratedModel':\n",
    "        # Instantiate the model\n",
    "        integrated_model_loaded = IntegratedModel(\n",
    "            deep_bilstm_model=deep_bilstm_model_loaded,\n",
    "            chord_model=chord_model_loaded,\n",
    "            filter_func=apply_window_filter,\n",
    "            window_size=128\n",
    "        ).to(device)\n",
    "        \n",
    "        # Load the model's state_dict\n",
    "        integrated_model_loaded.load_state_dict(torch.load(info['file'], map_location=device))\n",
    "        integrated_model_loaded.eval()\n",
    "        \n",
    "        # Evaluate the model\n",
    "        val_loss, val_acc = evaluate_model(integrated_model_loaded, valid_loader, nn.CrossEntropyLoss(ignore_index=0), device)\n",
    "        \n",
    "        # Store the evaluation results\n",
    "        evaluation_results[model_name] = {'val_loss': val_loss, 'val_acc': val_acc}\n",
    "        print(f\"{model_name} - Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "    else:\n",
    "        print(f\"Model '{model_name}' is not recognized for evaluation.\")\n",
    "\n",
    "# ==========================\n",
    "# 9. Validation Results Visualization\n",
    "# ==========================\n",
    "# Generate bar graphs to visualize validation loss and accuracy\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Validation Loss Visualization\n",
    "plt.subplot(1, 2, 1)\n",
    "model_names = list(evaluation_results.keys())\n",
    "val_losses = [evaluation_results[name]['val_loss'] for name in model_names]\n",
    "plt.bar(model_names, val_losses, color='skyblue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss of Best Models')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Validation Accuracy Visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "val_accs = [evaluation_results[name]['val_acc'] for name in model_names]\n",
    "plt.bar(model_names, val_accs, color='salmon')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy of Best Models')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================\n",
    "# 10. Visualization Function Call\n",
    "# ==========================\n",
    "def visualize_predictions(models, vocabs, valid_dataset, valid_loader, device, filter_func, window_size=128, sample_indices=[10, 11, 13, 14], max_steps=256):\n",
    "    \"\"\"\n",
    "    Visualizes predictions for multiple samples in the validation dataset.\n",
    "\n",
    "    Args:\n",
    "        models (dict): Dictionary with model names as keys and loaded model instances as values.\n",
    "        vocabs (dict): Dictionary with vocab names as keys and Vocab objects as values.\n",
    "        valid_dataset (Dataset): Validation dataset.\n",
    "        valid_loader (DataLoader): Validation DataLoader.\n",
    "        device (torch.device): Device to perform computations on.\n",
    "        filter_func (function): Function to apply filtering on key predictions.\n",
    "        window_size (int): Window size for filtering.\n",
    "        sample_indices (list): List of sample indices to visualize.\n",
    "        max_steps (int): Maximum number of time steps to display in the visualization.\n",
    "    \"\"\"\n",
    "    for sample_idx in sample_indices:\n",
    "        print(f\"\\nVisualizing Sample {sample_idx}\")\n",
    "        sample = valid_dataset[sample_idx]\n",
    "        visualize_sample_prediction(models, sample, sample_idx, vocabs, 64, 256)\n",
    "\n",
    "# Load models (assuming 'IntegratedModel' is the only one saved)\n",
    "loaded_models, loaded_vocabs = {'IntegratedModel': integrated_model}, vocabs\n",
    "\n",
    "# Visualize predictions for specific samples\n",
    "sample_indices = [10, 11, 13, 14]  # Change as needed\n",
    "visualize_predictions(\n",
    "    models=loaded_models,\n",
    "    vocabs=loaded_vocabs,\n",
    "    valid_dataset=valid_dataset,\n",
    "    valid_loader=valid_loader,\n",
    "    device=device,\n",
    "    filter_func=apply_window_filter,\n",
    "    window_size=128,\n",
    "    sample_indices=sample_indices,\n",
    "    max_steps=256\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c9532-48f4-4bc1-8615-d004dd2fe53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "def plot_confusion_matrix(N=24):\n",
    "    integrated_model_loaded.eval()\n",
    "    val_all_preds = []\n",
    "    val_all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            inputs = {\n",
    "                'note': batch['note'].to(device),\n",
    "                'bar': batch['bar'].to(device),\n",
    "                'tempo': batch['tempo'].to(device),\n",
    "                'velocity': batch['velocity'].to(device)\n",
    "            }\n",
    "            targets = batch['chord'].to(device)\n",
    "\n",
    "            outputs_chord, _ = integrated_model_loaded(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs_chord, dim=2)\n",
    "            mask = targets != 0  # exclude <pad> token\n",
    "\n",
    "            val_all_preds.extend(predicted[mask].cpu().numpy())\n",
    "            val_all_targets.extend(targets[mask].cpu().numpy())\n",
    "\n",
    "    # list to np\n",
    "    val_all_preds = np.array(val_all_preds)\n",
    "    val_all_targets = np.array(val_all_targets)\n",
    "\n",
    "    # exclude <unk> token\n",
    "    unk_token_idx = vocabs['vocab_chord'].token_to_idx.get('<unk>', None)\n",
    "    if unk_token_idx is not None:\n",
    "        valid_indices = (val_all_targets != unk_token_idx) & (val_all_preds != unk_token_idx)\n",
    "        val_all_preds = val_all_preds[valid_indices]\n",
    "        val_all_targets = val_all_targets[valid_indices]\n",
    "\n",
    "    chord_counts = Counter(val_all_targets)\n",
    "    # select top N chord\n",
    "    top_n = N\n",
    "    top_chords = chord_counts.most_common(top_n)\n",
    "    top_chord_indices = [chord for chord, _ in top_chords]\n",
    "\n",
    "    # idx to token name matching\n",
    "    idx_to_token = vocabs['vocab_chord'].idx_to_token\n",
    "    class_names = [idx_to_token[idx] for idx in top_chord_indices]\n",
    "\n",
    "    # confusion matrix masking for top N chord\n",
    "    mask = np.isin(val_all_targets, top_chord_indices)\n",
    "    val_all_preds_top = val_all_preds[mask]\n",
    "    val_all_targets_top = val_all_targets[mask]\n",
    "\n",
    "    # modify range properly\n",
    "    label_mapping = {old_idx: new_idx for new_idx, old_idx in enumerate(top_chord_indices)}\n",
    "    val_all_preds_mapped = np.array([label_mapping.get(idx, -1) for idx in val_all_preds_top])\n",
    "    val_all_targets_mapped = np.array([label_mapping.get(idx, -1) for idx in val_all_targets_top])\n",
    "\n",
    "    # excepction processing\n",
    "    valid_entries = (val_all_preds_mapped >= 0) & (val_all_targets_mapped >= 0)\n",
    "    val_all_preds_mapped = val_all_preds_mapped[valid_entries]\n",
    "    val_all_targets_mapped = val_all_targets_mapped[valid_entries]\n",
    "\n",
    "    # calculating confusion matrix\n",
    "    cm = confusion_matrix(val_all_targets_mapped, val_all_preds_mapped, labels=range(top_n))\n",
    "\n",
    "    # normalization\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # save\n",
    "    save_dir = 'results'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Normalized Confusion Matrix for Major & minor Chords')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=90)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save\n",
    "    save_path = os.path.join(save_dir, 'confusion_matrix.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Confusion matrix saved to {save_path}\")\n",
    "\n",
    "    # 그래프를 화면에 표시\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(N=24) # 24 for maj & min, 60 for all chord classes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (d2l)",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
