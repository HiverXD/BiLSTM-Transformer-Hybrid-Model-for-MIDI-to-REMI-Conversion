{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d22590f-8624-46da-afdc-d75d1f86023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# ==========================\n",
    "# Vocab Class Definition\n",
    "# ==========================\n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text tokens.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        # Create list of unique tokens\n",
    "        self.idx_to_token = ['<pad>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq and token != '<pad>'\n",
    "        ]\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self.token_to_idx.get('<unk>', 1)  # Return index 1 if '<unk>' is not in the vocabulary\n",
    "\n",
    "# ==========================\n",
    "# MIDI to Note Name Conversion Function\n",
    "# ==========================\n",
    "def midi_to_note_name(midi_numbers):\n",
    "    \"\"\"\n",
    "    Converts a list or array of MIDI pitch numbers to a list of note names.\n",
    "\n",
    "    Args:\n",
    "        midi_numbers (list or array): List or array of MIDI pitch numbers\n",
    "\n",
    "    Returns:\n",
    "        note_names (list): List of note names\n",
    "    \"\"\"\n",
    "    note_names_list = ['C', 'C#', 'D', 'D#', 'E', 'F',\n",
    "                       'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "    note_names = []\n",
    "    for num in midi_numbers:\n",
    "        if num == 0 or num == '<pad>':\n",
    "            note_names.append('<pad>')\n",
    "        else:\n",
    "            note_name = note_names_list[int(num) % 12]\n",
    "            note_names.append(note_name)\n",
    "    return note_names\n",
    "\n",
    "# ==========================\n",
    "# Dataset Class Definition\n",
    "# ==========================\n",
    "class MultiInputDataset(Dataset):\n",
    "    def __init__(self, note, bar, key):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with note, bar, and key tensors.\n",
    "\n",
    "        Args:\n",
    "            note (Tensor): Tensor of shape (num_samples, num_steps)\n",
    "            bar (Tensor): Tensor of shape (num_samples, num_steps)\n",
    "            key (Tensor): Tensor of shape (num_samples, num_steps)\n",
    "        \"\"\"\n",
    "        self.note = note\n",
    "        self.bar = bar\n",
    "        self.key = key\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.note.size(0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'note': self.note[idx],\n",
    "            'bar': self.bar[idx],\n",
    "            'key': self.key[idx]\n",
    "        }\n",
    "\n",
    "# ==========================\n",
    "# Base Model Class Definition\n",
    "# ==========================\n",
    "class MultiInputModelBase(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_sizes,       # Dictionary of vocab sizes for each feature\n",
    "        embed_dims,        # Dictionary of embedding dimensions for each feature\n",
    "        hidden_size=256,   # Hidden state size\n",
    "        num_layers=3, \n",
    "        num_classes=62,    # Number of target classes (key_vocab_size)\n",
    "        dropout=0.5\n",
    "    ):\n",
    "        super(MultiInputModelBase, self).__init__()\n",
    "        # Embedding layers (using note and bar features)\n",
    "        self.embed_note = nn.Embedding(vocab_sizes['note'], embed_dims['note'], padding_idx=0)\n",
    "        self.embed_bar = nn.Embedding(vocab_sizes['bar'], embed_dims['bar'], padding_idx=0)\n",
    "        \n",
    "        # Sum of embedding dimensions\n",
    "        self.total_embed_dim = sum(embed_dims.values())\n",
    "\n",
    "# ==========================\n",
    "# Deep LSTM Model Definition\n",
    "# ==========================\n",
    "class DenoiseLSTM(MultiInputModelBase):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_sizes, \n",
    "        embed_dims, \n",
    "        hidden_size=256,    # Hidden state size\n",
    "        num_layers=3, \n",
    "        num_classes=62,     # Number of target classes (key_vocab_size)\n",
    "        dropout=0.5\n",
    "    ):\n",
    "        super(DenoiseLSTM, self).__init__(vocab_sizes, embed_dims, hidden_size, num_layers, num_classes, dropout)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.total_embed_dim, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # inputs: dictionary of tensors\n",
    "        note = self.embed_note(inputs['note'])       # (batch_size, seq_len, embed_dim)\n",
    "        bar = self.embed_bar(inputs['bar'])\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        x = torch.cat([note, bar], dim=-1)  # (batch_size, seq_len, total_embed_dim)\n",
    "        \n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout_layer(out)\n",
    "        out = self.fc(out)     # (batch_size, seq_len, num_classes)\n",
    "        return out\n",
    "\n",
    "# ==========================\n",
    "# Deep BiLSTM Model Definition\n",
    "# ==========================\n",
    "class DenoiseBiLSTM(MultiInputModelBase):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_sizes, \n",
    "        embed_dims, \n",
    "        hidden_size=256,    # Hidden state size\n",
    "        num_layers=3, \n",
    "        num_classes=62,     # Number of target classes (key_vocab_size)\n",
    "        dropout=0.5\n",
    "    ):\n",
    "        super(DenoiseBiLSTM, self).__init__(vocab_sizes, embed_dims, hidden_size, num_layers, num_classes, dropout)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.total_embed_dim, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # inputs: dictionary of tensors\n",
    "        note = self.embed_note(inputs['note'])       # (batch_size, seq_len, embed_dim)\n",
    "        bar = self.embed_bar(inputs['bar'])\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        x = torch.cat([note, bar], dim=-1)  # (batch_size, seq_len, total_embed_dim)\n",
    "        \n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.dropout_layer(out)\n",
    "        out = self.fc(out)     # (batch_size, seq_len, num_classes)\n",
    "        return out\n",
    "\n",
    "# ==========================\n",
    "# Positional Encoding for Transformer\n",
    "# ==========================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2048):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# ==========================\n",
    "# Transformer Model Definition\n",
    "# ==========================\n",
    "class DenoiseTransformer(MultiInputModelBase):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_sizes, \n",
    "        embed_dims, \n",
    "        num_classes=62,     # Number of target classes (key_vocab_size)\n",
    "        nhead=8, \n",
    "        num_encoder_layers=4, \n",
    "        dim_feedforward=512, \n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super(DenoiseTransformer, self).__init__(vocab_sizes, embed_dims, num_classes=num_classes, dropout=dropout)\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(self.total_embed_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(self.total_embed_dim, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_encoder_layers)\n",
    "        self.fc = nn.Linear(self.total_embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs: dictionary of tensors\n",
    "        note = self.embed_note(inputs['note'])       # (batch_size, seq_len, embed_dim)\n",
    "        bar = self.embed_bar(inputs['bar'])\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        x = torch.cat([note, bar], dim=-1)  # (batch_size, seq_len, total_embed_dim)\n",
    "        \n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc(x)     # (batch_size, seq_len, num_classes)\n",
    "        return x\n",
    "\n",
    "# ==========================\n",
    "# Function to Find Best Model Files\n",
    "# ==========================\n",
    "def find_best_model_files(save_dir):\n",
    "    \"\"\"\n",
    "    Finds the best model files with the highest validation accuracy for each model type.\n",
    "\n",
    "    Args:\n",
    "        save_dir (str): Directory where the model files are saved\n",
    "\n",
    "    Returns:\n",
    "        best_models (dict): Dictionary containing the best model file info for each model type\n",
    "    \"\"\"\n",
    "    model_files = glob.glob(os.path.join(save_dir, '*.pt'))\n",
    "    best_models = {}\n",
    "    pattern = r'(.+)_epoch(\\d+)_valacc([\\d.]+)\\.pt'\n",
    "    for file in model_files:\n",
    "        filename = os.path.basename(file)\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            model_name, epoch, valacc = match.groups()\n",
    "            valacc = float(valacc)\n",
    "            if model_name not in best_models or valacc > best_models[model_name]['valacc']:\n",
    "                best_models[model_name] = {'file': file, 'epoch': int(epoch), 'valacc': valacc}\n",
    "    return best_models\n",
    "\n",
    "# ==========================\n",
    "# Function to Evaluate Model on Validation Set\n",
    "# ==========================\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model\n",
    "        dataloader (DataLoader): Validation DataLoader\n",
    "        criterion (nn.Module): Loss function\n",
    "        device (torch.device): Device to run the evaluation on\n",
    "\n",
    "    Returns:\n",
    "        avg_loss (float): Average validation loss\n",
    "        avg_acc (float): Average validation accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {\n",
    "                'note': batch['note'].to(device),\n",
    "                'bar': batch['bar'].to(device)\n",
    "            }\n",
    "            targets = batch['key'].to(device)\n",
    "            \n",
    "            outputs = model(inputs)  # (batch_size, seq_len, num_classes)\n",
    "            outputs_reshaped = outputs.view(-1, num_classes)\n",
    "            targets_reshaped = targets.view(-1)\n",
    "            \n",
    "            loss = criterion(outputs_reshaped, targets_reshaped)\n",
    "            val_loss += loss.item() * targets.size(0)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, dim=2)\n",
    "            val_correct += ((predicted == targets) & (targets != 0)).sum().item()\n",
    "            val_total += (targets != 0).sum().item()\n",
    "    \n",
    "    avg_loss = val_loss / len(dataloader.dataset)\n",
    "    avg_acc = val_correct / val_total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "# ==========================\n",
    "# Visualization Function for Sample Prediction (Modified)\n",
    "# ==========================\n",
    "def visualize_sample_prediction(models, sample, sample_index, vocab_note, vocab_key, start_step=0, end_step=64):\n",
    "    \"\"\"\n",
    "    Visualizes the model's prediction results for a specific sample in the validation dataset and saves it as a PNG file.\n",
    "\n",
    "    Args:\n",
    "        models (dict): Dictionary of trained models\n",
    "        sample (dict): Sample from the validation dataset\n",
    "        sample_index (int): Index of the sample in the validation dataset\n",
    "        vocab_note (Vocab): Vocabulary for note tokens\n",
    "        vocab_key (Vocab): Vocabulary for key tokens\n",
    "        start_step (int): Starting time step to visualize (inclusive)\n",
    "        end_step (int): Ending time step to visualize (exclusive)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    inputs = {\n",
    "        'note': sample['note'].unsqueeze(0).to(device),\n",
    "        'bar': sample['bar'].unsqueeze(0).to(device)\n",
    "    }\n",
    "    targets = sample['key'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Store prediction results\n",
    "    predictions = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)  # (1, seq_len, num_classes)\n",
    "            _, predicted = torch.max(outputs, dim=2)  # (1, seq_len)\n",
    "            predictions[name] = predicted.cpu().numpy().squeeze()  # (seq_len,)\n",
    "    \n",
    "    # Convert tokens to strings\n",
    "    targets_tokens = vocab_key.to_tokens(targets.cpu().numpy().squeeze())\n",
    "    note_tokens_indices = inputs['note'].cpu().numpy().squeeze()\n",
    "    note_tokens_sample = midi_to_note_name(note_tokens_indices)\n",
    "    \n",
    "    predicted_tokens = {name: vocab_key.to_tokens(pred) for name, pred in predictions.items()}\n",
    "    \n",
    "    # Determine sequence length\n",
    "    seq_length = len(targets_tokens)\n",
    "    start_step = max(0, start_step)\n",
    "    end_step = min(end_step, seq_length)\n",
    "    \n",
    "    if start_step >= end_step:\n",
    "        print(f\"Start time step {start_step} is greater than or equal to the sequence length {seq_length}. Skipping visualization.\")\n",
    "        return\n",
    "    \n",
    "    x = range(start_step, end_step)\n",
    "    \n",
    "    # Generate set of all tokens within the specified range\n",
    "    all_tokens_set = set(note_tokens_sample[start_step:end_step] + \n",
    "                         targets_tokens[start_step:end_step])\n",
    "    for pred in predicted_tokens.values():\n",
    "        all_tokens_set.update(pred[start_step:end_step])\n",
    "    \n",
    "    # Separate note names and key names\n",
    "    note_names_order = ['C', 'C#', 'D', 'D#', 'E', 'F',\n",
    "                        'F#', 'G', 'G#', 'A', 'A#', 'B', '<pad>']\n",
    "    note_names_in_tokens = [name for name in note_names_order if name in all_tokens_set]\n",
    "    \n",
    "    # Other tokens (e.g., '<unk>' etc.)\n",
    "    other_tokens = [token for token in all_tokens_set if token not in note_names_in_tokens]\n",
    "    other_tokens.sort()\n",
    "    \n",
    "    # Set the order of all tokens\n",
    "    sorted_all_tokens = note_names_in_tokens + other_tokens\n",
    "    \n",
    "    # Map tokens to numeric indices\n",
    "    token_to_index = {token: idx for idx, token in enumerate(sorted_all_tokens)}\n",
    "    \n",
    "    # Convert note, target, and predicted tokens to numeric indices\n",
    "    note_indices_plot = [token_to_index.get(token, -1) for token in note_tokens_sample[start_step:end_step]]\n",
    "    targets_indices_plot = [token_to_index.get(token, -1) for token in targets_tokens[start_step:end_step]]\n",
    "    \n",
    "    predicted_indices_plot = {}\n",
    "    for name, pred_tokens in predicted_tokens.items():\n",
    "        predicted_indices_plot[name] = [token_to_index.get(token, -1) for token in pred_tokens[start_step:end_step]]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    plt.plot(x, note_indices_plot, label='Input Note', alpha=0.5)\n",
    "    plt.plot(x, targets_indices_plot, label='Target Key', linestyle='-', color='black', linewidth=1)\n",
    "    \n",
    "    for name, indices in predicted_indices_plot.items():\n",
    "        plt.plot(x, indices, label=f'Predicted Key ({name})', linestyle='--')\n",
    "    \n",
    "    plt.title(f'Key Prediction (Sample {sample_index})')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Token')\n",
    "    \n",
    "    # Set y-axis ticks to token names\n",
    "    plt.yticks(ticks=range(len(sorted_all_tokens)), labels=sorted_all_tokens)\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Set directory to save the graph\n",
    "    save_dir = 'results'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Save the graph as PNG (including time step range in the filename)\n",
    "    save_path = os.path.join(save_dir, f'key_pred_result_{sample_index}_{start_step}_{end_step}.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Prediction result saved to {save_path}\")\n",
    "    \n",
    "    # Display the graph\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3211b6a2-9388-44b0-b5a5-0064a853172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Data Loading and Preprocessing\n",
    "# ==========================\n",
    "# Load data (use your provided paths)\n",
    "x_path = r\"example ... aug_x_pop.npy\"\n",
    "y_path = r\"example ... aug_y_pop.npy\"\n",
    "\n",
    "# Load data\n",
    "x = np.load(x_path, allow_pickle=True)  # shape: (num_samples, num_steps, 5)\n",
    "y = np.load(y_path, allow_pickle=True)  # shape: (num_samples, num_steps, 2)\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Separate features from x (using note, bar, and key)\n",
    "note_tokens = x[:, :, 0]      # (num_samples, num_steps)\n",
    "bar_tokens = x[:, :, 1]       # (num_samples, num_steps)\n",
    "key_tokens = x[:, :, 2]       # (num_samples, num_steps)\n",
    "\n",
    "# Create token lists for each feature\n",
    "note_tokens_list = note_tokens.tolist()\n",
    "bar_tokens_list = bar_tokens.tolist()\n",
    "key_tokens_list = key_tokens.tolist()\n",
    "\n",
    "# Create vocabularies with '<unk>' as a reserved token\n",
    "vocab_note = Vocab(tokens=note_tokens_list, min_freq=1, reserved_tokens=['<unk>'])\n",
    "vocab_bar = Vocab(tokens=bar_tokens_list, min_freq=1, reserved_tokens=['<unk>'])\n",
    "vocab_key = Vocab(tokens=key_tokens_list, min_freq=1, reserved_tokens=['<unk>'])\n",
    "\n",
    "print(f\"Vocabulary sizes:\")\n",
    "print(f\"Note: {len(vocab_note)}\")\n",
    "print(f\"Bar: {len(vocab_bar)}\")\n",
    "print(f\"Key: {len(vocab_key)}\")\n",
    "\n",
    "# Convert tokens to indices\n",
    "note_indices = [vocab_note[line] for line in note_tokens_list]\n",
    "bar_indices = [vocab_bar[line] for line in bar_tokens_list]\n",
    "key_indices = [vocab_key[line] for line in key_tokens_list]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "note_indices = np.array(note_indices)\n",
    "bar_indices = np.array(bar_indices)\n",
    "key_indices = np.array(key_indices)\n",
    "\n",
    "# Convert to Tensors\n",
    "note_tensor = torch.from_numpy(note_indices).long()         # (num_samples, num_steps)\n",
    "bar_tensor = torch.from_numpy(bar_indices).long()\n",
    "key_tensor = torch.from_numpy(key_indices).long()\n",
    "\n",
    "# ==========================\n",
    "# Dataset and DataLoader Preparation\n",
    "# ==========================\n",
    "# Create the dataset (using note and bar as inputs, key as target)\n",
    "dataset = MultiInputDataset(\n",
    "    note_tensor, bar_tensor, key_tensor\n",
    ")\n",
    "\n",
    "# Split the dataset (80% training, 20% validation)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Valid dataset size: {len(valid_dataset)}\")\n",
    "\n",
    "# Define DataLoader (batch size 32)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# ==========================\n",
    "# Model Initialization\n",
    "# ==========================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Target vocab size (key_vocab_size)\n",
    "num_classes = len(vocab_key)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Set vocab sizes and embedding dimensions for each feature\n",
    "vocab_sizes = {\n",
    "    'note': len(vocab_note),\n",
    "    'bar': len(vocab_bar)\n",
    "}\n",
    "\n",
    "embed_dims = {\n",
    "    'note': 64,      # Increased from 32 to 64\n",
    "    'bar': 16        # Increased from 8 to 16\n",
    "}\n",
    "\n",
    "# Update the model list (total of 3 models)\n",
    "models = {\n",
    "    'DeepLSTM': DenoiseLSTM(vocab_sizes=vocab_sizes, embed_dims=embed_dims, hidden_size=256, num_layers=3, num_classes=num_classes, dropout=0.5).to(device),\n",
    "    'DeepBiLSTM': DenoiseBiLSTM(vocab_sizes=vocab_sizes, embed_dims=embed_dims, hidden_size=256, num_layers=3, num_classes=num_classes, dropout=0.5).to(device),\n",
    "    'Transformer': DenoiseTransformer(vocab_sizes=vocab_sizes, embed_dims=embed_dims, num_classes=num_classes, nhead=8, num_encoder_layers=4, dim_feedforward=512, dropout=0.1).to(device),\n",
    "}\n",
    "\n",
    "# ==========================\n",
    "# Loss Function and Optimizers\n",
    "# ==========================\n",
    "# Define loss function (CrossEntropyLoss expects (N, C) and (N,))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Define optimizers (learning rate adjusted to 0.001)\n",
    "optimizers = {\n",
    "    name: optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    for name, model in models.items()\n",
    "}\n",
    "\n",
    "# Dictionaries to record loss and accuracy\n",
    "history = {\n",
    "    name: {'train_loss': [], 'valid_loss': [], 'train_acc': [], 'valid_acc': []}\n",
    "    for name in models.keys()\n",
    "}\n",
    "\n",
    "# Set directory to save models\n",
    "save_dir = r\"note_bar_to_key_models\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# ==========================\n",
    "# Training and Validation Loop\n",
    "# ==========================\n",
    "num_epochs = 50\n",
    "best_valid_loss = {name: float('inf') for name in models.keys()}\n",
    "start_time = time.time()  # Record training start time\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_start_time = time.time()  # Record epoch start time\n",
    "    print(f'\\nEpoch {epoch}/{num_epochs}')\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in train_loader:\n",
    "            # Extract inputs (note and bar)\n",
    "            inputs = {\n",
    "                'note': batch['note'].to(device),\n",
    "                'bar': batch['bar'].to(device)\n",
    "            }\n",
    "            targets = batch['key'].to(device)\n",
    "            \n",
    "            optimizer = optimizers[name]\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)  # (batch_size, seq_len, num_classes)\n",
    "            \n",
    "            # Reshape for loss: (batch_size * seq_len, num_classes)\n",
    "            outputs_reshaped = outputs.view(-1, num_classes)\n",
    "            targets_reshaped = targets.view(-1)  # (batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(outputs_reshaped, targets_reshaped)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * targets.size(0)\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=2)  # (batch_size, seq_len)\n",
    "            correct += ((predicted == targets) & (targets != 0)).sum().item()  # Exclude <pad> tokens\n",
    "            total += (targets != 0).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = correct / total\n",
    "        history[name]['train_loss'].append(epoch_loss)\n",
    "        history[name]['train_acc'].append(epoch_acc)\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                inputs = {\n",
    "                    'note': batch['note'].to(device),\n",
    "                    'bar': batch['bar'].to(device)\n",
    "                }\n",
    "                targets = batch['key'].to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                outputs_reshaped = outputs.view(-1, num_classes)\n",
    "                targets_reshaped = targets.view(-1)\n",
    "                \n",
    "                loss = criterion(outputs_reshaped, targets_reshaped)\n",
    "                val_loss += loss.item() * targets.size(0)\n",
    "                \n",
    "                _, predicted = torch.max(outputs, dim=2)\n",
    "                val_correct += ((predicted == targets) & (targets != 0)).sum().item()\n",
    "                val_total += (targets != 0).sum().item()\n",
    "        \n",
    "        epoch_val_loss = val_loss / valid_size\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        history[name]['valid_loss'].append(epoch_val_loss)\n",
    "        history[name]['valid_acc'].append(epoch_val_acc)\n",
    "        \n",
    "        # Periodic output\n",
    "        if epoch % 10 == 0 or epoch == 1 or epoch == num_epochs:\n",
    "            print(f'[{name}] Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, '\n",
    "                  f'Valid Loss: {epoch_val_loss:.4f}, Valid Acc: {epoch_val_acc:.4f}')\n",
    "        \n",
    "        # Save model\n",
    "        if epoch % 2 == 0 and epoch_val_loss < best_valid_loss[name]:\n",
    "            best_valid_loss[name] = epoch_val_loss\n",
    "            model_save_path = os.path.join(save_dir, f'{name}_epoch{epoch}_valacc{epoch_val_acc:.4f}.pt')\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model '{name}' saved at epoch {epoch} with validation accuracy {epoch_val_acc:.4f}\")\n",
    "    \n",
    "    # Calculate elapsed time and estimate remaining time\n",
    "    epoch_end_time = time.time()\n",
    "    elapsed_time = epoch_end_time - start_time\n",
    "    remaining_time = elapsed_time * (num_epochs / epoch) - elapsed_time\n",
    "    \n",
    "    # Print time information\n",
    "    print(f'Epoch {epoch}/{num_epochs} completed. Time elapsed: {elapsed_time:.2f} seconds. '\n",
    "          f'Estimated remaining time: {remaining_time:.2f} seconds.')\n",
    "\n",
    "# ==========================\n",
    "# Visualize Loss and Accuracy\n",
    "# ==========================\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Visualize Loss\n",
    "plt.subplot(2, 1, 1)\n",
    "for name in models.keys():\n",
    "    plt.plot(range(1, num_epochs + 1), history[name]['train_loss'], label=f'{name} Train', linestyle='-')\n",
    "    plt.plot(range(1, num_epochs + 1), history[name]['valid_loss'], label=f'{name} Valid', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Visualize Accuracy\n",
    "plt.subplot(2, 1, 2)\n",
    "for name in models.keys():\n",
    "    plt.plot(range(1, num_epochs + 1), history[name]['train_acc'], label=f'{name} Train', linestyle='-')\n",
    "    plt.plot(range(1, num_epochs + 1), history[name]['valid_acc'], label=f'{name} Valid', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================\n",
    "# Load Best Models and Evaluate\n",
    "# ==========================\n",
    "# Set directory where models are saved\n",
    "save_dir = r\"note_bar_to_key_models\"\n",
    "\n",
    "# Find best model files\n",
    "best_models_info = find_best_model_files(save_dir)\n",
    "\n",
    "# Load and evaluate each best model\n",
    "for model_name, info in best_models_info.items():\n",
    "    print(f\"Evaluating best model for {model_name}: {info['file']} (Epoch {info['epoch']}, Val Acc {info['valacc']:.4f})\")\n",
    "    \n",
    "    # Initialize the model\n",
    "    if model_name in models:\n",
    "        model = models[model_name]\n",
    "    else:\n",
    "        print(f\"Model {model_name} is not defined.\")\n",
    "        continue\n",
    "    \n",
    "    # Load saved state_dict\n",
    "    model.load_state_dict(torch.load(info['file'], map_location=device))\n",
    "    model.to(device)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_loss, val_acc = evaluate_model(model, valid_loader, criterion, device)\n",
    "    \n",
    "    # Update info\n",
    "    best_models_info[model_name]['valloss'] = val_loss\n",
    "    best_models_info[model_name]['valacc'] = val_acc\n",
    "    print(f\"{model_name} - Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# ==========================\n",
    "# Visualize Validation Results\n",
    "# ==========================\n",
    "# Generate bar graphs to visualize validation loss and accuracy\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Validation Loss Visualization\n",
    "plt.subplot(1, 2, 1)\n",
    "model_names = list(best_models_info.keys())\n",
    "val_losses = [best_models_info[name]['valloss'] for name in model_names]\n",
    "plt.bar(model_names, val_losses, color='skyblue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss of Best Models')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Validation Accuracy Visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "val_accs = [best_models_info[name]['valacc'] for name in model_names]\n",
    "plt.bar(model_names, val_accs, color='salmon')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy of Best Models')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================\n",
    "# Visualize Sample Predictions\n",
    "# ==========================\n",
    "# Select a sample from the validation dataset\n",
    "sample_index = 500  # You can change this index to visualize different samples\n",
    "sample = valid_dataset[sample_index]\n",
    "\n",
    "# Visualize sample prediction\n",
    "visualize_sample_prediction(models, sample, sample_index, vocab_note, vocab_key, start_step=0, end_step=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
