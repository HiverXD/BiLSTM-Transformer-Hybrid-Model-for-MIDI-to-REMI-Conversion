{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bea84f0-9d80-4a2d-907e-541c08bb9fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import collections\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import dill as pickle  # Using dill for serialization\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Vocab Class Definition\n",
    "# ==========================\n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text tokens.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        # Create list of unique tokens\n",
    "        self.idx_to_token = ['<pad>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq and token != '<pad>'\n",
    "        ]\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self.token_to_idx.get('<unk>', 1)  # Returns index 1 if '<unk>' is not in the vocabulary\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# MIDI to Note Name Conversion Function\n",
    "# ==========================\n",
    "def midi_to_note_name(midi_numbers):\n",
    "    \"\"\"\n",
    "    Converts a list or array of MIDI pitch numbers to a list of note names.\n",
    "    \n",
    "    Args:\n",
    "        midi_numbers (list or array): List or array of MIDI pitch numbers\n",
    "    \n",
    "    Returns:\n",
    "        note_names (list): List of note names\n",
    "    \"\"\"\n",
    "    NOTE_NAMES_LIST = ['C', 'C#', 'D', 'D#', 'E', 'F',\n",
    "                       'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "    note_names = []\n",
    "    for num in midi_numbers:\n",
    "        if num == 0 or num == '<pad>':\n",
    "            note_names.append('<pad>')\n",
    "        else:\n",
    "            note_name = NOTE_NAMES_LIST[int(num) % 12]\n",
    "            note_names.append(note_name)\n",
    "    return note_names\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Dataset Class Definition\n",
    "# ==========================\n",
    "class MultiInputDataset(Dataset):\n",
    "    def __init__(self, note, bar, key, chord):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with note, bar, key, and chord tensors.\n",
    "        \n",
    "        Args:\n",
    "            note (Tensor): Tensor of shape (num_samples, num_steps)\n",
    "            bar (Tensor): Tensor of shape (num_samples, num_steps)\n",
    "            key (Tensor): Tensor of shape (num_samples, num_steps)\n",
    "            chord (Tensor): Tensor of shape (num_samples, num_steps)\n",
    "        \"\"\"\n",
    "        self.note = note\n",
    "        self.bar = bar\n",
    "        self.key = key\n",
    "        self.chord = chord\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.note.size(0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'note': self.note[idx],\n",
    "            'bar': self.bar[idx],\n",
    "            'chord': self.chord[idx],\n",
    "            'key': self.key[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Base Model Class Definition\n",
    "# ==========================\n",
    "class MultiInputModelBase(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_sizes,       # Dictionary of vocab sizes for each feature\n",
    "        embed_dims,        # Dictionary of embedding dimensions for each feature\n",
    "        hidden_size=256,   # Hidden state size\n",
    "        num_layers=3, \n",
    "        num_classes=62,    # Number of target classes (key_vocab_size)\n",
    "        dropout=0.5\n",
    "    ):\n",
    "        super(MultiInputModelBase, self).__init__()\n",
    "        # Embedding layers (using note and bar features)\n",
    "        self.embed_note = nn.Embedding(vocab_sizes['note'], embed_dims['note'], padding_idx=0)\n",
    "        self.embed_bar = nn.Embedding(vocab_sizes['bar'], embed_dims['bar'], padding_idx=0)\n",
    "        \n",
    "        # Sum of embedding dimensions\n",
    "        self.total_embed_dim = sum(embed_dims.values())\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Deep LSTM Model Definition\n",
    "# ==========================\n",
    "class DenoiseLSTM(MultiInputModelBase):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_sizes, \n",
    "        embed_dims, \n",
    "        hidden_size=256,    # Hidden state size\n",
    "        num_layers=3, \n",
    "        num_classes=62,     # Number of target classes (key_vocab_size)\n",
    "        dropout=0.5\n",
    "    ):\n",
    "        super(DenoiseLSTM, self).__init__(vocab_sizes, embed_dims, hidden_size, num_layers, num_classes, dropout)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.total_embed_dim, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # inputs: dictionary of tensors\n",
    "        note = self.embed_note(inputs['note'])       # (batch_size, seq_len, embed_dim)\n",
    "        bar = self.embed_bar(inputs['bar'])\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        x = torch.cat([note, bar], dim=-1)  # (batch_size, seq_len, total_embed_dim)\n",
    "        \n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout_layer(out)\n",
    "        out = self.fc(out)     # (batch_size, seq_len, num_classes)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Deep BiLSTM Model Definition\n",
    "# ==========================\n",
    "class DenoiseBiLSTM(MultiInputModelBase):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_sizes, \n",
    "        embed_dims, \n",
    "        hidden_size=256,    # Hidden state size\n",
    "        num_layers=3, \n",
    "        num_classes=62,     # Number of target classes (key_vocab_size)\n",
    "        dropout=0.5\n",
    "    ):\n",
    "        super(DenoiseBiLSTM, self).__init__(vocab_sizes, embed_dims, hidden_size, num_layers, num_classes, dropout)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.total_embed_dim, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # inputs: dictionary of tensors\n",
    "        note = self.embed_note(inputs['note'])       # (batch_size, seq_len, embed_dim)\n",
    "        bar = self.embed_bar(inputs['bar'])\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        x = torch.cat([note, bar], dim=-1)  # (batch_size, seq_len, total_embed_dim)\n",
    "        \n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.dropout_layer(out)\n",
    "        out = self.fc(out)     # (batch_size, seq_len, num_classes)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Positional Encoding for Transformer\n",
    "# ==========================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1024):  # max_len set to 1024\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Transformer Model Definition\n",
    "# ==========================\n",
    "class DenoiseTransformer(MultiInputModelBase):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_sizes, \n",
    "        embed_dims, \n",
    "        num_classes=62,     # Number of target classes (key_vocab_size)\n",
    "        nhead=8, \n",
    "        num_encoder_layers=4, \n",
    "        dim_feedforward=512, \n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super(DenoiseTransformer, self).__init__(vocab_sizes, embed_dims, hidden_size=256, num_layers=3, num_classes=num_classes, dropout=dropout)\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(self.total_embed_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(self.total_embed_dim, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_encoder_layers)\n",
    "        self.fc = nn.Linear(self.total_embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs: dictionary of tensors\n",
    "        note = self.embed_note(inputs['note'])       # (batch_size, seq_len, embed_dim)\n",
    "        bar = self.embed_bar(inputs['bar'])\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        x = torch.cat([note, bar], dim=-1)  # (batch_size, seq_len, total_embed_dim)\n",
    "        \n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc(x)     # (batch_size, seq_len, num_classes)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Window Filter Function Definition\n",
    "# ==========================\n",
    "def apply_window_filter(key_hat, window_size=64):\n",
    "    \"\"\"\n",
    "    Applies a window filter to the predicted key sequence.\n",
    "    \n",
    "    Args:\n",
    "        key_hat (1D numpy array): Array of predicted key indices.\n",
    "        window_size (int): Size of the window (e.g., 64).\n",
    "    \n",
    "    Returns:\n",
    "        filtered_key_hat (1D numpy array): Filtered key sequence.\n",
    "    \"\"\"\n",
    "    key_hat = np.array(key_hat)\n",
    "    output = key_hat.copy()\n",
    "    num_steps = len(key_hat)\n",
    "    num_windows = num_steps // window_size\n",
    "\n",
    "    for n in range(num_windows):\n",
    "        start = n * window_size\n",
    "        end = (n + 1) * window_size\n",
    "        window = key_hat[start:end]\n",
    "        window_non_pad = window[window != 0]\n",
    "\n",
    "        if len(window_non_pad) == 0:\n",
    "            continue  # If only <pad> exists in the window\n",
    "\n",
    "        counts = np.bincount(window_non_pad)\n",
    "        most_freq = np.argmax(counts)\n",
    "        freq = counts[most_freq]\n",
    "\n",
    "        # Check if the most frequent value is unique\n",
    "        if np.sum(counts == freq) > 1:\n",
    "            # If multiple values have the same highest frequency, attempt to resolve using the next window\n",
    "            replacement = None\n",
    "            for m in range(n + 1, num_windows):\n",
    "                next_start = m * window_size\n",
    "                next_end = (m + 1) * window_size\n",
    "                next_window = key_hat[next_start:next_end]\n",
    "                next_window_non_pad = next_window[next_window != 0]\n",
    "                if len(next_window_non_pad) == 0:\n",
    "                    continue\n",
    "                next_counts = np.bincount(next_window_non_pad)\n",
    "                next_most_freq = np.argmax(next_counts)\n",
    "                next_freq = next_counts[next_most_freq]\n",
    "                if np.sum(next_counts == next_freq) == 1:\n",
    "                    replacement = next_most_freq\n",
    "                    break\n",
    "            if replacement is not None:\n",
    "                # Replace non-pad keys in the current window with the replacement\n",
    "                window_filtered = np.where(window != 0, replacement, window)\n",
    "                output[start:end] = window_filtered\n",
    "            else:\n",
    "                # If replacement cannot be determined, retain the original window\n",
    "                continue\n",
    "        else:\n",
    "            # Replace non-pad keys in the window with the most frequent key\n",
    "            replacement = most_freq\n",
    "            window_filtered = np.where(window != 0, replacement, window)\n",
    "            output[start:end] = window_filtered\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Model Saving and Loading Functions\n",
    "# ==========================\n",
    "def save_models_and_vocabs(models, vocabs, save_dir):\n",
    "    \"\"\"\n",
    "    Saves multiple Vocab objects and models to the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        models (dict): Dictionary with model names as keys and model instances as values.\n",
    "        vocabs (dict): Dictionary with vocab names as keys and Vocab objects as values.\n",
    "        save_dir (str): Directory path to save the models and Vocab objects.\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        print(f\"Directory created: {save_dir}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {save_dir}\")\n",
    "    \n",
    "    # Save Vocab objects using dill\n",
    "    vocab_save_path = os.path.join(save_dir, 'vocabs.pkl')\n",
    "    with open(vocab_save_path, 'wb') as f:\n",
    "        pickle.dump(vocabs, f)\n",
    "        print(f\"Vocab objects saved: {vocab_save_path}\")\n",
    "    \n",
    "    # Save each model\n",
    "    for name, model in models.items():\n",
    "        save_path = os.path.join(save_dir, f\"{name}.pth\")  # e.g., DeepLSTM.pth\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Model '{name}' saved: {save_path}\")\n",
    "    \n",
    "    print(\"All Vocab objects and models have been successfully saved.\")\n",
    "\n",
    "\n",
    "def load_models_and_vocabs(model_classes, save_dir, device):\n",
    "    \"\"\"\n",
    "    Loads saved Vocab objects and multiple models from the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        model_classes (dict): Dictionary with model names as keys and model classes as values.\n",
    "        save_dir (str): Directory path where models and Vocab objects are saved.\n",
    "        device (torch.device): Device to load the models onto (CPU or GPU).\n",
    "    \n",
    "    Returns:\n",
    "        loaded_models (dict): Dictionary with model names as keys and loaded model instances as values.\n",
    "        vocabs (dict): Dictionary with vocab names as keys and loaded Vocab objects as values.\n",
    "    \"\"\"\n",
    "    # Load Vocab objects using dill\n",
    "    vocab_save_path = os.path.join(save_dir, 'vocabs.pkl')\n",
    "    with open(vocab_save_path, 'rb') as f:\n",
    "        vocabs = pickle.load(f)\n",
    "        print(\"Vocab objects loaded.\")\n",
    "    \n",
    "    # Initialize a dictionary to store loaded models\n",
    "    loaded_models = {}\n",
    "    \n",
    "    # Load each model\n",
    "    for name, model_class in model_classes.items():\n",
    "        # Instantiate the model\n",
    "        if name == 'DeepLSTM':\n",
    "            model = model_class(\n",
    "                vocab_sizes={'note': len(vocabs['vocab_note']), 'bar': len(vocabs['vocab_bar'])},\n",
    "                embed_dims={'note': 64, 'bar': 16},\n",
    "                hidden_size=256, num_layers=3, num_classes=len(vocabs['vocab_key']), dropout=0.5\n",
    "            )\n",
    "        elif name == 'DeepBiLSTM':\n",
    "            model = model_class(\n",
    "                vocab_sizes={'note': len(vocabs['vocab_note']), 'bar': len(vocabs['vocab_bar'])},\n",
    "                embed_dims={'note': 64, 'bar': 16},\n",
    "                hidden_size=256, num_layers=3, num_classes=len(vocabs['vocab_key']), dropout=0.5\n",
    "            )\n",
    "        elif name == 'Transformer':\n",
    "            model = model_class(\n",
    "                vocab_sizes={'note': len(vocabs['vocab_note']), 'bar': len(vocabs['vocab_bar'])},\n",
    "                embed_dims={'note': 64, 'bar': 16},\n",
    "                num_classes=len(vocabs['vocab_key']),\n",
    "                nhead=8, num_encoder_layers=4, dim_feedforward=512, dropout=0.1\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model name: {name}\")\n",
    "        \n",
    "        # Move model to the specified device\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load the model's state_dict\n",
    "        load_path = os.path.join(save_dir, f\"{name}.pth\")\n",
    "        if os.path.exists(load_path):\n",
    "            model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "            print(f\"Model '{name}' loaded from {load_path}\")\n",
    "        else:\n",
    "            print(f\"Model file for '{name}' does not exist at {load_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Add the loaded model to the dictionary\n",
    "        loaded_models[name] = model\n",
    "    \n",
    "    print(\"All models and Vocab objects have been successfully loaded.\")\n",
    "    return loaded_models, vocabs\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Evaluation and Visualization Functions\n",
    "# ==========================\n",
    "def compute_validation_accuracy(models, valid_loader, device, filter_func, window_size=64):\n",
    "    \"\"\"\n",
    "    Computes the validation accuracy for each model after applying a filter.\n",
    "    \n",
    "    Args:\n",
    "        models (dict): Dictionary with model names as keys and loaded model instances as values.\n",
    "        valid_loader (DataLoader): Validation DataLoader.\n",
    "        device (torch.device): Device to perform computations on.\n",
    "        filter_func (function): Function to apply filtering (e.g., apply_window_filter).\n",
    "        window_size (int): Size of the window for filtering.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy_dict (dict): Dictionary with model names as keys and accuracy scores as values.\n",
    "    \"\"\"\n",
    "    accuracy_dict = {name: {'correct': 0, 'total': 0} for name in models.keys()}\n",
    "    \n",
    "    for batch in valid_loader:\n",
    "        note = batch['note'].to(device)\n",
    "        bar = batch['bar'].to(device)\n",
    "        key = batch['key'].to(device)\n",
    "        chord = batch['chord'].to(device)\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                inputs = {\n",
    "                    'note': note,\n",
    "                    'bar': bar,\n",
    "                    'chord': chord  # Assuming chord is used; if not, replace with zeros or appropriate tensor\n",
    "                }\n",
    "                outputs = model(inputs)  # (batch_size, seq_len, num_classes)\n",
    "                _, predicted = torch.max(outputs, dim=2)  # (batch_size, seq_len)\n",
    "                predicted = predicted.cpu().numpy()  # Convert to numpy array\n",
    "                \n",
    "                # Apply filter\n",
    "                filtered_predicted = np.array([filter_func(pred, window_size) for pred in predicted])\n",
    "                \n",
    "                # Convert back to tensor\n",
    "                filtered_predicted = torch.from_numpy(filtered_predicted).long().to(device)  # (batch_size, seq_len)\n",
    "                \n",
    "                # Compute accuracy excluding <pad> tokens\n",
    "                correct = ((filtered_predicted == key) & (key != 0)).sum().item()\n",
    "                total = (key != 0).sum().item()\n",
    "                \n",
    "                accuracy_dict[name]['correct'] += correct\n",
    "                accuracy_dict[name]['total'] += total\n",
    "    \n",
    "    # Calculate final accuracy scores\n",
    "    for name in accuracy_dict:\n",
    "        if accuracy_dict[name]['total'] > 0:\n",
    "            accuracy_dict[name] = accuracy_dict[name]['correct'] / accuracy_dict[name]['total']\n",
    "        else:\n",
    "            accuracy_dict[name] = 0.0\n",
    "    \n",
    "    return accuracy_dict\n",
    "\n",
    "\n",
    "def visualize_predictions_with_accuracy(models, vocabs, valid_dataset, valid_loader, device, filter_func, window_size=64, sample_idx=500, max_steps=256):\n",
    "    \"\"\"\n",
    "    Visualizes the predictions of multiple models for a specific validation sample and displays overall validation accuracy.\n",
    "    \n",
    "    Args:\n",
    "        models (dict): Dictionary with model names as keys and loaded model instances as values.\n",
    "        vocabs (dict): Dictionary with vocab names as keys and Vocab objects as values.\n",
    "        valid_dataset (Dataset): Validation dataset.\n",
    "        valid_loader (DataLoader): Validation DataLoader.\n",
    "        device (torch.device): Device to perform computations on.\n",
    "        filter_func (function): Function to apply filtering (e.g., apply_window_filter).\n",
    "        window_size (int): Size of the window for filtering.\n",
    "        sample_idx (int): Index of the sample in the validation dataset to visualize.\n",
    "        max_steps (int): Maximum number of time steps to display in the visualization.\n",
    "    \"\"\"\n",
    "    # Compute overall validation accuracy\n",
    "    accuracy_dict = compute_validation_accuracy(models, valid_loader, device, filter_func, window_size)\n",
    "    \n",
    "    # Select the specific sample\n",
    "    sample = valid_dataset[sample_idx]\n",
    "    inputs = {\n",
    "        'note': sample['note'].unsqueeze(0).to(device),\n",
    "        'bar': sample['bar'].unsqueeze(0).to(device),\n",
    "        'chord': sample['chord'].unsqueeze(0).to(device)  # Assuming chord is used\n",
    "    }\n",
    "    targets = sample['key'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Store predictions from each model\n",
    "    predictions = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)  # (1, seq_len, num_classes)\n",
    "            _, predicted = torch.max(outputs, dim=2)  # (1, seq_len)\n",
    "            predictions[name] = predicted.cpu().numpy().squeeze()  # (seq_len,)\n",
    "    \n",
    "    # Convert targets and notes to tokens\n",
    "    targets_tokens = vocabs['vocab_key'].to_tokens(targets.cpu().numpy().squeeze())\n",
    "    note_tokens_sample = vocabs['vocab_note'].to_tokens(inputs['note'].cpu().numpy().squeeze())\n",
    "    \n",
    "    # Convert predictions to tokens\n",
    "    predicted_tokens = {name: vocabs['vocab_key'].to_tokens(pred) for name, pred in predictions.items()}\n",
    "    \n",
    "    # Apply filter to predictions\n",
    "    filtered_predictions = {name: apply_window_filter(pred, window_size) for name, pred in predictions.items()}\n",
    "    filtered_predicted_tokens = {name: vocabs['vocab_key'].to_tokens(pred) for name, pred in filtered_predictions.items()}\n",
    "    \n",
    "    # Determine the number of steps to display\n",
    "    num_display = len(targets_tokens) if max_steps is None else min(len(targets_tokens), max_steps)\n",
    "    \n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    for idx, (name, pred_tokens) in enumerate(filtered_predicted_tokens.items(), 1):\n",
    "        plt.subplot(len(models), 1, idx)\n",
    "        plt.plot(range(num_display), note_tokens_sample[:num_display], label='Input Note', alpha=0.5)\n",
    "        plt.plot(range(num_display), targets_tokens[:num_display], label='Target Key', alpha=0.8)\n",
    "        plt.plot(range(num_display), pred_tokens[:num_display], label='Predicted Key', alpha=0.8)\n",
    "        plt.title(f'Prediction using {name}')\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Token')\n",
    "        plt.legend()\n",
    "        # Display validation accuracy on the plot\n",
    "        plt.text(0.99, 0.95, f'Validation Accuracy: {accuracy_dict[name]:.4f}', \n",
    "                 horizontalalignment='right',\n",
    "                 verticalalignment='top',\n",
    "                 transform=plt.gca().transAxes,\n",
    "                 fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53120c-5a4e-4321-9b4b-f16928977f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import collections\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the above definitions are in a module or have been run in the same script\n",
    "\n",
    "# ==========================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ==========================\n",
    "# Define data paths\n",
    "x_path = r\"example path ... aug_x_pop.npy\"\n",
    "y_path = r\"example path ... aug_y_pop.npy\"\n",
    "\n",
    "# Load data\n",
    "x = np.load(x_path, allow_pickle=True)  # shape: (num_samples, num_steps, 5)\n",
    "y = np.load(y_path, allow_pickle=True)  # shape: (num_samples, num_steps, 2)\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Separate features from x (using note, bar, and key)\n",
    "note_tokens = x[:, :, 0]      # (num_samples, num_steps)\n",
    "bar_tokens = x[:, :, 1]       # (num_samples, num_steps)\n",
    "\n",
    "# Separate features from y\n",
    "key_tokens = x[:, :, 2]       # (num_samples, num_steps) # key is at x[:,:,2]\n",
    "chord_tokens = y[:, :, 0]     # (num_samples, num_steps) # assuming chord is y[:,:,0]\n",
    "\n",
    "# Create token lists for each feature\n",
    "note_tokens_list = note_tokens.tolist()\n",
    "bar_tokens_list = bar_tokens.tolist()\n",
    "key_tokens_list = key_tokens.tolist()\n",
    "chord_tokens_list = chord_tokens.tolist()\n",
    "\n",
    "# Create vocabularies with '<unk>' as a reserved token\n",
    "vocabs = {\n",
    "    'vocab_note': Vocab(tokens=note_tokens_list, min_freq=1, reserved_tokens=['<unk>']),\n",
    "    'vocab_bar': Vocab(tokens=bar_tokens_list, min_freq=1, reserved_tokens=['<unk>']),\n",
    "    'vocab_key': Vocab(tokens=key_tokens_list, min_freq=1, reserved_tokens=['<unk>']),\n",
    "    'vocab_chord': Vocab(tokens=chord_tokens_list, min_freq=1, reserved_tokens=['<unk>'])\n",
    "}\n",
    "\n",
    "print(f\"Vocabulary sizes:\")\n",
    "for vocab_name, vocab in vocabs.items():\n",
    "    print(f\"{vocab_name}: {len(vocab)}\")\n",
    "\n",
    "# Convert tokens to indices\n",
    "note_indices = [vocabs['vocab_note'][line] for line in note_tokens_list]\n",
    "bar_indices = [vocabs['vocab_bar'][line] for line in bar_tokens_list]\n",
    "key_indices = [vocabs['vocab_key'][line] for line in key_tokens_list]\n",
    "chord_indices = [vocabs['vocab_chord'][line] for line in chord_tokens_list]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "note_indices = np.array(note_indices)\n",
    "bar_indices = np.array(bar_indices)\n",
    "key_indices = np.array(key_indices)\n",
    "chord_indices = np.array(chord_indices)\n",
    "\n",
    "# Convert to Tensors\n",
    "note_tensor = torch.from_numpy(note_indices).long()         # (num_samples, num_steps)\n",
    "bar_tensor = torch.from_numpy(bar_indices).long()           # (num_samples, num_steps)\n",
    "key_tensor = torch.from_numpy(key_indices).long()           # (num_samples, num_steps)\n",
    "chord_tensor = torch.from_numpy(chord_indices).long()       # (num_samples, num_steps)\n",
    "\n",
    "# ==========================\n",
    "# 2. Dataset and DataLoader Preparation\n",
    "# ==========================\n",
    "# Create the dataset (using note, bar, chord as inputs, key as target)\n",
    "dataset = MultiInputDataset(\n",
    "    note_tensor, bar_tensor, key_tensor, chord_tensor\n",
    ")\n",
    "\n",
    "# Split the dataset (80% training, 20% validation)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Valid dataset size: {len(valid_dataset)}\")\n",
    "\n",
    "# Define DataLoader (batch size 32)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# ==========================\n",
    "# 3. Model Initialization\n",
    "# ==========================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Target vocab size (key_vocab_size)\n",
    "num_classes = len(vocabs['vocab_key'])\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Set vocab sizes and embedding dimensions for each feature\n",
    "vocab_sizes = {\n",
    "    'note': len(vocabs['vocab_note']),\n",
    "    'bar': len(vocabs['vocab_bar'])\n",
    "}\n",
    "\n",
    "embed_dims = {\n",
    "    'note': 64,      # Increased from 32 to 64\n",
    "    'bar': 16        # Increased from 8 to 16\n",
    "}\n",
    "\n",
    "# Define model classes mapping\n",
    "model_classes = {\n",
    "    'DeepLSTM': DenoiseLSTM,\n",
    "    'DeepBiLSTM': DenoiseBiLSTM,\n",
    "    'Transformer': DenoiseTransformer\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'DeepLSTM': DenoiseLSTM(vocab_sizes=vocab_sizes, embed_dims=embed_dims, hidden_size=256, num_layers=3, num_classes=num_classes, dropout=0.5).to(device),\n",
    "    'DeepBiLSTM': DenoiseBiLSTM(vocab_sizes=vocab_sizes, embed_dims=embed_dims, hidden_size=256, num_layers=3, num_classes=num_classes, dropout=0.5).to(device),\n",
    "    'Transformer': DenoiseTransformer(vocab_sizes=vocab_sizes, embed_dims=embed_dims, num_classes=num_classes, nhead=8, num_encoder_layers=4, dim_feedforward=512, dropout=0.1).to(device),\n",
    "}\n",
    "\n",
    "# ==========================\n",
    "# 4. Loss Function and Optimizers\n",
    "# ==========================\n",
    "# Define loss function (CrossEntropyLoss expects (N, C) and (N,))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Define optimizers (learning rate adjusted to 0.001)\n",
    "optimizers = {\n",
    "    name: optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    for name, model in models.items()\n",
    "}\n",
    "\n",
    "# Dictionaries to record loss and accuracy\n",
    "history = {\n",
    "    name: {'train_loss': [], 'valid_loss': [], 'train_acc': [], 'valid_acc': []}\n",
    "    for name in models.keys()\n",
    "}\n",
    "\n",
    "# Set directory to save models\n",
    "save_dir = r\"note_bar_to_key_models\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    print(f\"Directory created: {save_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {save_dir}\")\n",
    "\n",
    "# ==========================\n",
    "# 5. Training and Validation Loop\n",
    "# ==========================\n",
    "num_epochs = 50\n",
    "best_valid_loss = {name: float('inf') for name in models.keys()}\n",
    "start_time = time.time()  # Record training start time\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_start_time = time.time()  # Record epoch start time\n",
    "    print(f'\\nEpoch {epoch}/{num_epochs}')\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in train_loader:\n",
    "            # Extract inputs (note, bar, chord)\n",
    "            inputs = {\n",
    "                'note': batch['note'].to(device),\n",
    "                'bar': batch['bar'].to(device),\n",
    "                'chord': batch['chord'].to(device)\n",
    "            }\n",
    "            targets = batch['key'].to(device)\n",
    "            \n",
    "            optimizer = optimizers[name]\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)  # (batch_size, seq_len, num_classes)\n",
    "            \n",
    "            # Reshape for loss: (batch_size * seq_len, num_classes)\n",
    "            outputs_reshaped = outputs.view(-1, num_classes)\n",
    "            targets_reshaped = targets.view(-1)  # (batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(outputs_reshaped, targets_reshaped)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * targets.size(0)\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=2)  # (batch_size, seq_len)\n",
    "            correct += ((predicted == targets) & (targets != 0)).sum().item()  # Exclude <pad> tokens\n",
    "            total += (targets != 0).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = correct / total\n",
    "        history[name]['train_loss'].append(epoch_loss)\n",
    "        history[name]['train_acc'].append(epoch_acc)\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                inputs = {\n",
    "                    'note': batch['note'].to(device),\n",
    "                    'bar': batch['bar'].to(device),\n",
    "                    'chord': batch['chord'].to(device)\n",
    "                }\n",
    "                targets = batch['key'].to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                outputs_reshaped = outputs.view(-1, num_classes)\n",
    "                targets_reshaped = targets.view(-1)\n",
    "                \n",
    "                loss = criterion(outputs_reshaped, targets_reshaped)\n",
    "                val_loss += loss.item() * targets.size(0)\n",
    "                \n",
    "                _, predicted = torch.max(outputs, dim=2)\n",
    "                val_correct += ((predicted == targets) & (targets != 0)).sum().item()\n",
    "                val_total += (targets != 0).sum().item()\n",
    "        \n",
    "        epoch_val_loss = val_loss / valid_size\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        history[name]['valid_loss'].append(epoch_val_loss)\n",
    "        history[name]['valid_acc'].append(epoch_val_acc)\n",
    "        \n",
    "        # Periodic output\n",
    "        if epoch % 10 == 0 or epoch == 1 or epoch == num_epochs:\n",
    "            print(f'[{name}] Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, '\n",
    "                  f'Valid Loss: {epoch_val_loss:.4f}, Valid Acc: {epoch_val_acc:.4f}')\n",
    "        \n",
    "        # Save model if validation loss has improved\n",
    "        if epoch % 2 == 0 and epoch_val_loss < best_valid_loss[name]:\n",
    "            best_valid_loss[name] = epoch_val_loss\n",
    "            model_save_path = os.path.join(save_dir, f'{name}_epoch{epoch}_valacc{epoch_val_acc:.4f}.pt')\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model '{name}' saved at epoch {epoch} with validation accuracy {epoch_val_acc:.4f}\")\n",
    "    \n",
    "    # Calculate elapsed time and estimate remaining time\n",
    "    epoch_end_time = time.time()\n",
    "    elapsed_time = epoch_end_time - start_time\n",
    "    remaining_time = elapsed_time * (num_epochs / epoch) - elapsed_time\n",
    "    \n",
    "    # Print time information\n",
    "    print(f'Epoch {epoch}/{num_epochs} completed. Time elapsed: {elapsed_time:.2f} seconds. '\n",
    "          f'Estimated remaining time: {remaining_time:.2f} seconds.')\n",
    "\n",
    "# ==========================\n",
    "# 6. Visualize Loss and Accuracy\n",
    "# ==========================\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Visualize Loss\n",
    "plt.subplot(2, 1, 1)\n",
    "for name in models.keys():\n",
    "    plt.plot(range(1, num_epochs + 1), history[name]['train_loss'], label=f'{name} Train', linestyle='-')\n",
    "    plt.plot(range(1, num_epochs + 1), history[name]['valid_loss'], label=f'{name} Valid', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Visualize Accuracy\n",
    "plt.subplot(2, 1, 2)\n",
    "for name in models.keys():\n",
    "    plt.plot(range(1, num_epochs + 1), history[name]['train_acc'], label=f'{name} Train', linestyle='-')\n",
    "    plt.plot(range(1, num_epochs + 1), history[name]['valid_acc'], label=f'{name} Valid', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================\n",
    "# 7. Model Loading and Evaluation\n",
    "# ==========================\n",
    "# Define a mapping from model names to their classes\n",
    "model_classes_mapping = {\n",
    "    'DeepLSTM': DenoiseLSTM,\n",
    "    'DeepBiLSTM': DenoiseBiLSTM,\n",
    "    'Transformer': DenoiseTransformer\n",
    "}\n",
    "\n",
    "# Define the directory where models are saved\n",
    "save_dir = r\"\" # use your path\n",
    "\n",
    "# Find best model files based on validation accuracy\n",
    "best_models = find_best_model_files(save_dir)\n",
    "\n",
    "# Initialize a dictionary to store evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "# Evaluate each best model\n",
    "for model_name, info in best_models.items():\n",
    "    print(f\"Evaluating best model for {model_name}: {info['file']} (Epoch {info['epoch']}, Val Acc {info['valacc']:.4f})\")\n",
    "    \n",
    "    # Check if the model name exists in the mapping\n",
    "    if model_name in model_classes_mapping:\n",
    "        model_class = model_classes_mapping[model_name]\n",
    "        # Instantiate the model\n",
    "        if model_name == 'Transformer':\n",
    "            model = model_class(\n",
    "                vocab_sizes={'note': len(vocabs['vocab_note']), 'bar': len(vocabs['vocab_bar'])},\n",
    "                embed_dims={'note': 64, 'bar': 16},\n",
    "                num_classes=len(vocabs['vocab_key']),\n",
    "                nhead=8, num_encoder_layers=4, dim_feedforward=512, dropout=0.1\n",
    "            )\n",
    "        else:\n",
    "            model = model_class(\n",
    "                vocab_sizes={'note': len(vocabs['vocab_note']), 'bar': len(vocabs['vocab_bar'])},\n",
    "                embed_dims={'note': 64, 'bar': 16},\n",
    "                hidden_size=256, num_layers=3, num_classes=len(vocabs['vocab_key']), dropout=0.5\n",
    "            )\n",
    "        model.to(device)\n",
    "    else:\n",
    "        print(f\"Model class for '{model_name}' is not defined.\")\n",
    "        continue\n",
    "    \n",
    "    # Load the model's state_dict\n",
    "    model.load_state_dict(torch.load(info['file'], map_location=device))\n",
    "    model.to(device)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_loss, val_acc = evaluate_model(model, valid_loader, nn.CrossEntropyLoss(ignore_index=0), device)\n",
    "    \n",
    "    # Store the evaluation results\n",
    "    evaluation_results[model_name] = {'val_loss': val_loss, 'val_acc': val_acc}\n",
    "    print(f\"{model_name} - Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# ==========================\n",
    "# 8. Visualize Validation Results\n",
    "# ==========================\n",
    "# Generate bar graphs to visualize validation loss and accuracy\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Validation Loss Visualization\n",
    "plt.subplot(1, 2, 1)\n",
    "model_names = list(evaluation_results.keys())\n",
    "val_losses = [evaluation_results[name]['val_loss'] for name in model_names]\n",
    "plt.bar(model_names, val_losses, color='skyblue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss of Best Models')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Validation Accuracy Visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "val_accs = [evaluation_results[name]['val_acc'] for name in model_names]\n",
    "plt.bar(model_names, val_accs, color='salmon')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy of Best Models')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================\n",
    "# 9. Visualization Function Call\n",
    "# ==========================\n",
    "# Load models and vocabs\n",
    "loaded_models, loaded_vocabs = load_models_and_vocabs(model_classes_mapping, save_dir, device)\n",
    "\n",
    "# Define window size for filtering\n",
    "window_size = 128  # Set the window size as needed\n",
    "\n",
    "# Visualize predictions for a specific sample\n",
    "sample_idx = 15  # Index of the sample to visualize\n",
    "max_steps = 256  # Maximum number of time steps to display\n",
    "\n",
    "visualize_predictions_with_accuracy(\n",
    "    models=loaded_models,\n",
    "    vocabs=vocabs,\n",
    "    valid_dataset=valid_dataset,\n",
    "    valid_loader=valid_loader,\n",
    "    device=device,\n",
    "    filter_func=apply_window_filter,\n",
    "    window_size=window_size,\n",
    "    sample_idx=sample_idx,\n",
    "    max_steps=max_steps\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
